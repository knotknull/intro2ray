{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Ray Train\n",
    "\n",
    "This notebook will walk you through the basics of distributed training with Ray Train and PyTorch.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "<b> Here is the roadmap for this notebook:</b>\n",
    "\n",
    "<ul>\n",
    "    <li><b>Part 1:</b> Single GPU PyTorch</li>\n",
    "    <li><b>Part 2:</b> Overview of the training loop in Ray Train</li>\n",
    "    <li><b>Part 3:</b> Migrating the model to Ray Train</li>\n",
    "    <li><b>Part 4:</b> Migrating the dataset to Ray Train</li>\n",
    "    <li><b>Part 5:</b> Reporting metrics and checkpoints</li>\n",
    "    <li><b>Part 6:</b> Launching the distributed training job</li>\n",
    "    <li><b>Part 7:</b> Accessing training results</li>\n",
    "    <li><b>Part 8:</b> Ray Train in production </li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os   \n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "import datetime\n",
    "\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchmetrics\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.models import resnet18\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import ToTensor, Normalize, Compose\n",
    "\n",
    "import ray\n",
    "from ray.train import ScalingConfig, RunConfig\n",
    "from ray.train.torch import TorchTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Single GPU PyTorch\n",
    "\n",
    "We will start by fitting a `ResNet18` model to an `MNIST` dataset.\n",
    "\n",
    "Here is a diagram visualizing the single GPU training process:\n",
    "\n",
    "<img src=\"https://anyscale-public-materials.s3.us-west-2.amazonaws.com/ray-ai-libraries/diagrams/single_gpu_pytorch_v3.png\" width=\"800\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At a high level, here is how our training loop in PyTorch looks like: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop_torch(num_epochs: int = 2, batch_size: int = 128, local_path: str = \"./checkpoints\"):\n",
    "    # Model, Loss, Optimizer\n",
    "    criterion = CrossEntropyLoss()\n",
    "    model = load_model_torch()\n",
    "    optimizer = Adam(model.parameters(), lr=1e-5)\n",
    "\n",
    "    # Initialize the metric \n",
    "    acc = torchmetrics.Accuracy(task=\"multiclass\", num_classes=10).to(\"cuda\")\n",
    "\n",
    "    # Load the data loader\n",
    "    data_loader = build_data_loader_torch(batch_size=batch_size)\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        for images, labels in data_loader:\n",
    "            # Move the data to the GPU\n",
    "            images, labels = images.to(\"cuda\"), labels.to(\"cuda\")\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "\n",
    "            # Compute the loss\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            # Update the weights\n",
    "            optimizer.step()\n",
    "\n",
    "            # Update the metric\n",
    "            acc(outputs, labels)\n",
    "        \n",
    "        # Report the metrics\n",
    "        metrics = report_metrics_torch(loss=loss, accuracy=acc.compute(), epoch=epoch)\n",
    "        \n",
    "        # Reset the metric\n",
    "        acc.reset()\n",
    "\n",
    "        # Save the checkpoint and metrics\n",
    "        Path(local_path).mkdir(parents=True, exist_ok=True)\n",
    "        save_checkpoint_and_metrics_torch(metrics=metrics, model=model, local_path=local_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first start by defining how to build and load our model on a single GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_resnet18():\n",
    "    model = resnet18(num_classes=10)\n",
    "    model.conv1 = torch.nn.Conv2d(\n",
    "        in_channels=1, # grayscale MNIST images\n",
    "        out_channels=64,\n",
    "        kernel_size=(7, 7),\n",
    "        stride=(2, 2),\n",
    "        padding=(3, 3),\n",
    "        bias=False,\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "def load_model_torch() -> torch.nn.Module:\n",
    "    model = build_resnet18()\n",
    "    # Move to the single GPU device\n",
    "    model.to(\"cuda\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 9.91M/9.91M [00:00<00:00, 27.2MB/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 28.9k/28.9k [00:00<00:00, 1.44MB/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1.65M/1.65M [00:00<00:00, 12.8MB/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4.54k/4.54k [00:00<00:00, 10.1MB/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = MNIST(root=\"./data\", train=True, download=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's display the first 10 images, with the corresponding labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/archive/ray/course/intro2ray/.venv/lib/python3.12/site-packages/torchvision/datasets/mnist.py:76: UserWarning: train_data has been renamed data\n",
      "  warnings.warn(\"train_data has been renamed data\")\n",
      "/data/archive/ray/course/intro2ray/.venv/lib/python3.12/site-packages/torchvision/datasets/mnist.py:66: UserWarning: train_labels has been renamed targets\n",
      "  warnings.warn(\"train_labels has been renamed targets\")\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x200 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axs = plt.subplots(1, 10, figsize=(20, 2))\n",
    "\n",
    "for i in range(10):\n",
    "    axs[i].imshow(dataset.train_data[i], cmap=\"gray\")\n",
    "    axs[i].axis(\"off\")\n",
    "    axs[i].set_title(dataset.train_labels[i].item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also define a data loader to load our data in batches and apply transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_data_loader_torch(batch_size: int) -> DataLoader:\n",
    "    transform = Compose([ToTensor(), Normalize((0.5,), (0.5,))])\n",
    "    dataset = MNIST(root=\"./data\", train=True, download=True, transform=transform)\n",
    "    train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "    return train_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will compute and report metrics via a simple print statement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def report_metrics_torch(loss: torch.Tensor, accuracy: torch.Tensor, epoch: int) -> None:\n",
    "    metrics = {\"loss\": loss.item(), \"epoch\": epoch, \"accuracy\": accuracy.item()}\n",
    "    print(metrics)\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To save the checkpoint we will make use of a local directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint_and_metrics_torch(metrics: dict[str, float], model: torch.nn.Module, local_path: str) -> None:\n",
    "    # Save the metrics\n",
    "    with open(os.path.join(local_path, \"metrics.csv\"), \"a\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(metrics.values())\n",
    "\n",
    "    # Save the model\n",
    "    checkpoint_path = os.path.join(local_path, \"model.pt\")\n",
    "    torch.save(model.state_dict(), checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can schedule the training loop on a single GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.17373433709144592, 'epoch': 0, 'accuracy': 0.80931156873703}\n",
      "{'loss': 0.12736190855503082, 'epoch': 1, 'accuracy': 0.9513054490089417}\n"
     ]
    }
   ],
   "source": [
    "timestamp = datetime.datetime.now(datetime.UTC).strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "local_path = f\"/mnt/local_storage/single_gpu_mnist/torch_{timestamp}/\"\n",
    "\n",
    "train_loop_torch(\n",
    "    num_epochs=2, \n",
    "    local_path=local_path\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect the produced checkpoints and metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 43732\n",
      "-rw-rw-r-- 1 map map       82 Apr 10 09:18 metrics.csv\n",
      "-rw-rw-r-- 1 map map 44773570 Apr 10 09:18 model.pt\n"
     ]
    }
   ],
   "source": [
    "!ls -l {local_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>epoch</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.173734</td>\n",
       "      <td>0</td>\n",
       "      <td>0.809312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.127362</td>\n",
       "      <td>1</td>\n",
       "      <td>0.951305</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       loss  epoch  accuracy\n",
       "0  0.173734      0  0.809312\n",
       "1  0.127362      1  0.951305"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics = pd.read_csv(\n",
    "    os.path.join(local_path, \"metrics.csv\"),\n",
    "    header=None,\n",
    "    names=[\"loss\", \"epoch\", \"accuracy\"],\n",
    ")\n",
    "\n",
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also load our produced model checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model = build_resnet18()\n",
    "loaded_model.load_state_dict(torch.load(os.path.join(local_path, \"model.pt\")))\n",
    "loaded_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can the proceed to generate predictions on the first 10 images of the MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/archive/ray/course/intro2ray/.venv/lib/python3.12/site-packages/torchvision/datasets/mnist.py:76: UserWarning: train_data has been renamed data\n",
      "  warnings.warn(\"train_data has been renamed data\")\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x200 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axs = plt.subplots(1, 10, figsize=(20, 2))\n",
    "for i in range(10):\n",
    "    axs[i].imshow(dataset.train_data[i], cmap=\"gray\")\n",
    "    axs[i].axis(\"off\")\n",
    "    with torch.no_grad():\n",
    "        normalized = Normalize((0.5,), (0.5,))(ToTensor()(dataset[i][0]))\n",
    "        prediction = loaded_model(normalized.unsqueeze(0)).argmax()\n",
    "    axs[i].set_title(prediction.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributed Data Parallel Training with Ray Train and PyTorch\n",
    "\n",
    "Let's now consider the case where we have a very large dataset of images that would take a long time to train on a single GPU. \n",
    "\n",
    "We would now like to scale this training job to run on multiple GPUs. \n",
    "\n",
    "Here is a diagram visualizing the desired distributed data-parallel training process:\n",
    "\n",
    "<img src=\"https://anyscale-public-materials.s3.us-west-2.amazonaws.com/ray-ai-libraries/diagrams/multi_gpu_pytorch_v4.png\" width=\"1000\" >\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Overview of the training loop in Ray Train\n",
    "\n",
    "Let's see how this data-parallel training loop will look like with Ray Train and PyTorch.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop_ray_train(config: dict):  # pass in hyperparameters in config\n",
    "    criterion = CrossEntropyLoss()\n",
    "    # Use Ray Train to wrap the model with DistributedDataParallel\n",
    "    model = load_model_ray_train()\n",
    "    optimizer = Adam(model.parameters(), lr=1e-5)\n",
    "    \n",
    "    # Calculate the batch size for each worker\n",
    "    global_batch_size = config[\"global_batch_size\"]\n",
    "    batch_size = global_batch_size // ray.train.get_context().get_world_size()\n",
    "    # Use Ray Train to wrap the data loader as a DistributedSampler\n",
    "    data_loader = build_data_loader_ray_train(batch_size=batch_size) \n",
    "    \n",
    "    acc = torchmetrics.Accuracy(task=\"multiclass\", num_classes=10).to(model.device)\n",
    "\n",
    "    for epoch in range(config[\"num_epochs\"]):\n",
    "        # Ensure data is on the correct device\n",
    "        data_loader.sampler.set_epoch(epoch)\n",
    "\n",
    "        for images, labels in data_loader: # images, labels are now sharded across the workers\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward() # gradients are now accumulated across the workers\n",
    "            optimizer.step()\n",
    "            acc(outputs, labels)\n",
    "\n",
    "        accuracy = acc.compute() # accuracy is now aggregated across the workers\n",
    "\n",
    "        # Use Ray Train to report metrics\n",
    "        metrics = print_metrics_ray_train(loss, accuracy, epoch)\n",
    "\n",
    "        # Use Ray Train to save checkpoint and metrics\n",
    "        save_checkpoint_and_metrics_ray_train(model, metrics)\n",
    "        acc.reset() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure scale and GPUs\n",
    "Outside of our training function, we create a `ScalingConfig` object to configure:\n",
    "\n",
    "- `num_workers`: The number of distributed training worker processes.\n",
    "- `use_gpu`: Whether each worker should use a GPU (or CPU).\n",
    "\n",
    "\n",
    "See [docs on configuring scale and GPUs](https://docs.ray.io/en/latest/train/user-guides/using-gpus.html) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaling_config = ScalingConfig(num_workers=2, use_gpu=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a high-level architecture of how Ray Train works:\n",
    "\n",
    "<img src=\"https://docs.ray.io/en/latest/_images/overview.png\" width=600>\n",
    "\n",
    "Here are some key points:\n",
    "- The scaling config specifies the number of training workers.\n",
    "- A trainer actor process is launched that oversees the training workers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will dive deep in to the steps:\n",
    "- Migrating the model to Ray Train\n",
    "- Migrating the dataset to Ray Train\n",
    "- Reporting metrics and checkpoints\n",
    "- Launching a training job\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Migrating the model to Ray Train\n",
    "\n",
    "Use the `ray.train.torch.prepare_model()` utility function to:\n",
    "\n",
    "- Automatically move your model to the correct device.\n",
    "- Wrap the model in pytorch's `DistributedDataParallel`.\n",
    "\n",
    "To learn more about the `prepare_model()` function, see the [API reference](https://docs.ray.io/en/latest/train/api/doc/ray.train.torch.prepare_model.html#ray-train-torch-prepare-model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_ray_train() -> torch.nn.Module:\n",
    "    model = build_resnet18()\n",
    "    model = ray.train.torch.prepare_model(model) # Instead of model = model.to(\"cuda\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Migrating the dataset to Ray Train\n",
    "\n",
    "Use the `ray.train.torch.prepare_data_loader()` utility function, to:\n",
    "\n",
    "- Automatically moves the batches to the right device.\n",
    "- Wrap the data loader with pytorch's `DistributedSampler`.\n",
    "\n",
    "To learn more about the `prepare_data_loader()` function, see the [API reference](https://docs.ray.io/en/latest/train/api/doc/ray.train.torch.prepare_data_loader.html#ray-train-torch-prepare-data-loader)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_data_loader_ray_train(batch_size: int) -> DataLoader:\n",
    "    transform = Compose([ToTensor(), Normalize((0.5,), (0.5,))])\n",
    "    train_data = MNIST(root=\"./data\", train=True, download=True, transform=transform)\n",
    "    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "    # Add DistributedSampler to the DataLoader\n",
    "    train_loader = ray.train.torch.prepare_data_loader(train_loader)\n",
    "    return train_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b> Note</b> that this step isn’t necessary if you are integrating your Ray Train implementaiton with Ray Data.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Reporting checkpoints and metrics\n",
    "\n",
    "To monitor progress, we can continue to print/log metrics as before. This time we chose to only do so for the first worker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_metrics_ray_train(\n",
    "    loss: torch.Tensor, accuracy: torch.Tensor, epoch: int\n",
    ") -> None:\n",
    "    metrics = {\"loss\": loss.item(), \"accuracy\": accuracy.item(), \"epoch\": epoch}\n",
    "    if ray.train.get_context().get_world_rank() == 0:\n",
    "        print(metrics)\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we will report intermediate metrics and checkpoints using the `ray.train.report` utility function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint_and_metrics_ray_train(\n",
    "    model: torch.nn.Module, metrics: dict[str, float]\n",
    ") -> None:\n",
    "    with tempfile.TemporaryDirectory() as temp_checkpoint_dir:\n",
    "        torch.save(\n",
    "            model.module.state_dict(),  # note the .module to unwrap the DistributedDataParallel\n",
    "            os.path.join(temp_checkpoint_dir, \"model.pt\"),\n",
    "        )\n",
    "        ray.train.report(  # use ray.train.report to save the metrics and checkpoint\n",
    "            metrics,  # train.report will only save worker rank 0's metrics\n",
    "            checkpoint=ray.train.Checkpoint.from_directory(temp_checkpoint_dir),\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the lifecycle of a checkpoint from being created using a local path to being uploaded to persistent storage.\n",
    "\n",
    "<img src=\"https://docs.ray.io/en/latest/_images/checkpoint_lifecycle.png\" width=800>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given it is the same model across all workers, we can instead only build the checkpoint on worker of rank 0. Note that we will still need to call `ray.train.report` on all workers to ensure that the training loop is synchronized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint_and_metrics_ray_train(\n",
    "    model: torch.nn.Module, metrics: dict[str, float]\n",
    ") -> None:\n",
    "    with tempfile.TemporaryDirectory() as temp_checkpoint_dir:\n",
    "        checkpoint = None\n",
    "        if ray.train.get_context().get_world_rank() == 0:\n",
    "            torch.save(\n",
    "                model.module.state_dict(), os.path.join(temp_checkpoint_dir, \"model.pt\")\n",
    "            )\n",
    "            checkpoint = ray.train.Checkpoint.from_directory(temp_checkpoint_dir)\n",
    "\n",
    "        ray.train.report(\n",
    "            metrics,\n",
    "            checkpoint=checkpoint,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For an in-depth guide on saving checkpoints and metrics, see the [docs](https://docs.ray.io/en/latest/train/user-guides/checkpoints.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Launching the distributed training job\n",
    "\n",
    "Here is the desired data-parallel training diagram, but now using Ray Train.\n",
    "\n",
    "<img src=\"https://anyscale-public-materials.s3.us-west-2.amazonaws.com/ray-ai-libraries/diagrams/multi_gpu_pytorch_annotated_v5.png\" width=\"1000\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Let's proceed to launch the distributed training job."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure persistent storage\n",
    "Create a `RunConfig` object to specify the path where results (including checkpoints and artifacts) will be saved.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_path = \"/mnt/cluster_storage/ray-summit-2024-training/\"\n",
    "run_config = RunConfig(storage_path=storage_path, name=\"distributed-mnist-resnet18\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now launch a distributed training job with a `TorchTrainer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = TorchTrainer(\n",
    "    train_loop_ray_train,\n",
    "    scaling_config=scaling_config,\n",
    "    run_config=run_config,\n",
    "    train_loop_config={\"num_epochs\": 2, \"global_batch_size\": 128},\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling `trainer.fit()` will start the run and block until it completes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Access the training results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training completes, a `Result` object is returned which contains information about the training run, including the metrics and checkpoints reported during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check the metrics produced by the training job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.metrics_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also take the latest checkpoint and load it to inspect the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt = result.checkpoint\n",
    "with ckpt.as_directory() as ckpt_dir:\n",
    "    model_path = os.path.join(ckpt_dir, \"model.pt\")\n",
    "    loaded_model_ray_train = build_resnet18()\n",
    "    state_dict = torch.load(model_path, map_location=torch.device('cpu'), weights_only=True)\n",
    "    loaded_model_ray_train.load_state_dict(state_dict)\n",
    "    loaded_model_ray_train.eval()\n",
    "\n",
    "loaded_model_ray_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To learn more about the training results, see this [docs](https://docs.ray.io/en/latest/train/user-guides/results.html) on inspecting the training results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then proceed to generate predictions using the loaded model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 10, figsize=(20, 2))\n",
    "\n",
    "for i in range(10):\n",
    "    axs[i].imshow(dataset.train_data[i], cmap=\"gray\")\n",
    "    axs[i].axis(\"off\")\n",
    "    with torch.no_grad():\n",
    "        normalized = Normalize((0.5,), (0.5,))(ToTensor()(dataset[i][0]))\n",
    "        prediction = loaded_model_ray_train(normalized.unsqueeze(0)).argmax()\n",
    "    axs[i].set_title(prediction.item())\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "### Activity: Update the training loop to compute AUROC\n",
    "\n",
    "1. Update the training loop `train_loop_ray_train` to compute the AUROC metric.\n",
    "2. Update the `print_metrics_ray_train` function to include the AUROC metric.\n",
    "3. Save the AUROC metric in the `save_checkpoint_and_metrics_ray_train` function.\n",
    "\n",
    "Use the following code snippets to guide you:\n",
    "\n",
    "```python\n",
    "# Hint: Update the print function to include AUROC\n",
    "def print_metrics_ray_train(...):\n",
    "    ...\n",
    "\n",
    "def train_loop_ray_train(config):\n",
    "    # Hint: Update the training loop to compute AUROC\n",
    "\n",
    "trainer = TorchTrainer(\n",
    "    train_loop_ray_train,\n",
    "    scaling_config=scaling_config,\n",
    "    run_config=run_config,\n",
    "    train_loop_config={\"num_epochs\": 2, \"global_batch_size\": 128},\n",
    ")\n",
    "result = trainer.fit()\n",
    "result.metrics_dataframe\n",
    "```\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your solution here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "<details>\n",
    "\n",
    "<summary> Click here to see the solution </summary>\n",
    "\n",
    "```python\n",
    "def print_metrics_ray_train(loss, accuracy, auroc):\n",
    "    metrics = {\n",
    "        \"loss\": loss.item(),\n",
    "        \"accuracy\": accuracy.item(),\n",
    "        \"auroc\": auroc.item(),\n",
    "    }\n",
    "    if ray.train.get_context().get_world_rank() == 0:\n",
    "        print(\n",
    "            f\"Loss: {loss.item()}, Accuracy: {accuracy.item()}, AUROC: {auroc.item()}\"\n",
    "        )\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def train_loop_ray_train(config):\n",
    "    criterion = CrossEntropyLoss()\n",
    "    model = load_model_ray_train()\n",
    "    optimizer = Adam(model.parameters(), lr=1e-5)\n",
    "\n",
    "    global_batch_size = config[\"global_batch_size\"]\n",
    "    batch_size = global_batch_size // ray.train.get_context().get_world_size()\n",
    "    data_loader = build_data_loader_ray_train(batch_size=batch_size)\n",
    "\n",
    "    acc = torchmetrics.Accuracy(task=\"multiclass\", num_classes=10).to(model.device)\n",
    "    # Add AUROC metric\n",
    "    auroc = torchmetrics.AUROC(task=\"multiclass\", num_classes=10).to(model.device)\n",
    "\n",
    "    for epoch in range(config[\"num_epochs\"]):\n",
    "        data_loader.sampler.set_epoch(epoch)\n",
    "\n",
    "        for images, labels in data_loader:\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            acc(outputs, labels)\n",
    "            auroc(outputs, labels)\n",
    "\n",
    "        metrics = print_metrics_ray_train(\n",
    "            loss, acc.compute(), auroc.compute()\n",
    "        )\n",
    "        save_checkpoint_and_metrics_ray_train(model, metrics)\n",
    "        acc.reset()\n",
    "        auroc.reset()\n",
    "\n",
    "trainer = TorchTrainer(\n",
    "    train_loop_ray_train,\n",
    "    scaling_config=scaling_config,\n",
    "    run_config=run_config,\n",
    "    train_loop_config={\"num_epochs\": 2, \"global_batch_size\": 128},\n",
    ")\n",
    "result = trainer.fit()\n",
    "print(result.metrics_dataframe)\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Ray Train in Production\n",
    "\n",
    "Here are some use-cases of using Ray Train in production:\n",
    "1. Canva uses Ray Train + Ray Data to cut down Stable Diffusion training costs by 3.7x. Read this [Anyscale blog post here](https://www.anyscale.com/blog/scalable-and-cost-efficient-stable-diffusion-pre-training-with-ray) and the [Canva  case study here](https://www.anyscale.com/resources/case-study/how-canva-built-a-modern-ai-platform-using-anyscale)\n",
    "2. Anyscale uses Ray Train + Deepspeed to finetune language models. Read more [here](https://github.com/ray-project/ray/tree/master/doc/source/templates/04_finetuning_llms_with_deepspeed).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell for file cleanup \n",
    "!rm -rf /mnt/cluster_storage/single_gpu_mnist"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  },
  "orphan": true,
  "vscode": {
   "interpreter": {
    "hash": "a8c1140d108077f4faeb76b2438f85e4ed675f93d004359552883616a1acd54c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
