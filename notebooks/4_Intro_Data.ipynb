{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to Ray Data\n",
    "\n",
    "This notebook will provide an overview of Ray Data and how to use it to load, and transform data in a distributed manner.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b> Here is the roadmap for this notebook:</b>\n",
    "<ul>\n",
    "    <li><b>Part 1:</b> When to use Ray Data</a></li>\n",
    "    <li><b>Part 2:</b> Loading Data</a></li>\n",
    "    <li><b>Part 3:</b> Transforming Data</a></li>\n",
    "    <li><b>Part 4:</b> Materializing Data</a></li>\n",
    "    <li><b>Part 5:</b> Data Operations: Grouping, Aggregation, and Shuffling</a></li>\n",
    "    <li><b>Part 6:</b> Persisting Data</a></li>\n",
    "    <li><b>Part 7:</b> Ray Data in Production</a></li>\n",
    "</ul>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torchvision.transforms import Compose, ToTensor, Normalize\n",
    "\n",
    "import ray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. When to use Ray Data\n",
    "\n",
    "Use Ray Data to load and preprocess data for distributed ML workloads. Compared to other loading solutions, Datasets are more flexible and provide [higher overall performance](https://www.anyscale.com/blog/why-third-generation-ml-platforms-are-more-performant). Ray Data is especially performant when needing to run pre-processing in a **streaming fashion** across a **large dataset** on a **heterogeneous cluster of CPUs and GPUs**.\n",
    "\n",
    "\n",
    "Use Datasets as a last-mile bridge from storage or ETL pipeline outputs to distributed applications and libraries in Ray. \n",
    "\n",
    "<img src='https://docs.ray.io/en/releases-2.34.0/_images/dataset-loading-1.svg' width=60%/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Loading Data\n",
    "\n",
    "Datasets uses Ray tasks to read data from remote storage. When reading from a file-based datasource (e.g., S3, GCS), it creates a number of read tasks proportional to the number of CPUs in the cluster. Each read task reads its assigned files and produces an output block:\n",
    "\n",
    "<img src=\"https://anyscale-public-materials.s3.us-west-2.amazonaws.com/ray-summit/rag-app/dataset-read-cropped-v2.svg\" width=\"500px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load some `MNIST` data from s3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           PRE 0/\n",
      "                           PRE 1/\n",
      "                           PRE 2/\n",
      "                           PRE 3/\n",
      "                           PRE 4/\n",
      "                           PRE 5/\n",
      "                           PRE 6/\n",
      "                           PRE 7/\n",
      "                           PRE 8/\n",
      "                           PRE 9/\n"
     ]
    }
   ],
   "source": [
    "# Here is our dataset it contains 50 images per class\n",
    "!aws s3 ls s3://anyscale-public-materials/ray-ai-libraries/mnist/50_per_index/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the `read_images` function to load the image data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-15 21:39:54,355\tINFO streaming_executor.py:108 -- Starting execution of Dataset. Full logs are in /tmp/ray/session_2025-04-15_21-36-25_446080_3579077/logs/ray-data\n",
      "2025-04-15 21:39:54,356\tINFO streaming_executor.py:109 -- Execution plan of Dataset: InputDataBuffer[Input] -> TaskPoolMapOperator[ReadImage]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d19e4dcf9c4b46a19016d3e9ea8e92bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running 0: 0.00 row [00:00, ? row/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5be1c7176e59439c9b77f213f1c3954c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "- ReadImage 1: 0.00 row [00:00, ? row/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b25af6d7f924e3aaea6208fe74aee63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dataset(\n",
       "   num_rows=500,\n",
       "   schema={image: numpy.ndarray(shape=(28, 28), dtype=uint8), path: string}\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ds = ray.data.read_images(\"s3://anyscale-public-materials/ray-ai-libraries/mnist/50_per_index/\", include_paths=True)\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refer to the [Input/Output docs](https://docs.ray.io/en/latest/data/api/input_output.html) for a comprehensive list of read functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "A Dataset consists of a list of Ray object references to *blocks*. Having multiple blocks in a dataset allows for parallel transformation and ingest.\n",
    "\n",
    "The following figure visualizes a tabular dataset with three blocks, each block holding 1000 rows each:\n",
    "\n",
    "<img src='https://docs.ray.io/en/releases-2.6.1/_images/dataset-arch.svg' width=50%/>\n",
    "\n",
    "Since a Dataset is just a list of Ray object references, it can be freely passed between Ray tasks, actors, and libraries like any other object reference. This flexibility is a unique characteristic of Ray Datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Transforming Data\n",
    "\n",
    "Ray Data can use either Ray tasks or Ray actors to transform datasets. Using actors allows for expensive state initialization (e.g., for GPU-based tasks) to be cached.\n",
    "\n",
    "Ray Data simplifies general purpose parallel GPU and CPU compute in Ray. \n",
    "\n",
    "Here is a sample data pipeline for streaming image data across a classification and segmentation model on a heterogenous cluster of CPUs and GPUs.\n",
    "\n",
    "<img src='https://docs.ray.io/en/releases-2.6.1/_images/stream-example.png' width=60%/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To transform data, we can use the `map_batches` API. This API allows us to apply a transformation to each batch of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-15 22:08:29,574\tINFO streaming_executor.py:108 -- Starting execution of Dataset. Full logs are in /tmp/ray/session_2025-04-15_21-36-25_446080_3579077/logs/ray-data\n",
      "2025-04-15 22:08:29,574\tINFO streaming_executor.py:109 -- Execution plan of Dataset: InputDataBuffer[Input] -> TaskPoolMapOperator[ReadImage]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bca5c58140544390a2b1a0728dabcbfc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running 0: 0.00 row [00:00, ? row/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b08af5881514e02aae9e15512ee3d2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "- ReadImage 1: 0.00 row [00:00, ? row/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8c239596f324961b0baa2bd4077d248",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "MapBatches(normalize)\n",
       "+- Dataset(\n",
       "      num_rows=500,\n",
       "      schema={image: numpy.ndarray(shape=(28, 28), dtype=uint8), path: string}\n",
       "   )"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def normalize(\n",
    "    batch: dict[str, np.ndarray], min_: float, max_: float\n",
    ") -> dict[str, np.ndarray]:\n",
    "    transform = Compose([ToTensor(), Normalize((0.5,), (0.5,))])\n",
    "    batch[\"image\"] = [transform(image) for image in batch[\"image\"]]\n",
    "    return batch\n",
    "\n",
    "\n",
    "ds_normalized = ds.map_batches(normalize, fn_kwargs={\"min_\": 0, \"max_\": 255})\n",
    "ds_normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execution mode\n",
    "\n",
    "Most transformations are **lazy**. They don't execute until you write a dataset to storage or decide to materialize/consume the dataset.\n",
    "\n",
    "To materialize a very small subset of the data, you can use the `take_batch` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-15 22:11:59,258\tINFO streaming_executor.py:108 -- Starting execution of Dataset. Full logs are in /tmp/ray/session_2025-04-15_21-36-25_446080_3579077/logs/ray-data\n",
      "2025-04-15 22:11:59,259\tINFO streaming_executor.py:109 -- Execution plan of Dataset: InputDataBuffer[Input] -> TaskPoolMapOperator[ReadImage->MapBatches(normalize)] -> LimitOperator[limit=10]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33b821755cc549b087da48e3fa4c0246",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running 0: 0.00 row [00:00, ? row/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e8d078889da4937a89189a773573f48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "- ReadImage->MapBatches(normalize) 1: 0.00 row [00:00, ? row/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73bfce21be3c4812918d115351e8ca11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "- limit=10 2: 0.00 row [00:00, ? row/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-15 22:12:05,855\tERROR worker.py:420 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::ReadImage->MapBatches(normalize)()\u001b[39m (pid=3591297, ip=192.168.99.98)\n",
      "    for b_out in map_transformer.apply_transform(iter(blocks), ctx):\n",
      "  File \"/data/archive/ray/course/intro2ray/.venv/lib/python3.12/site-packages/ray/data/_internal/execution/operators/map_transformer.py\", line 536, in __call__\n",
      "    yield output_buffer.next()\n",
      "          ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/data/archive/ray/course/intro2ray/.venv/lib/python3.12/site-packages/ray/data/_internal/output_buffer.py\", line 124, in next\n",
      "    block_to_yield = self._buffer.build()\n",
      "                     ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/data/archive/ray/course/intro2ray/.venv/lib/python3.12/site-packages/ray/data/_internal/delegating_block_builder.py\", line 68, in build\n",
      "    return self._builder.build()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/data/archive/ray/course/intro2ray/.venv/lib/python3.12/site-packages/ray/data/_internal/table_block.py\", line 143, in build\n",
      "    return self._concat_tables(tables)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/data/archive/ray/course/intro2ray/.venv/lib/python3.12/site-packages/ray/data/_internal/arrow_block.py\", line 142, in _concat_tables\n",
      "    return transform_pyarrow.concat(tables, promote_types=True)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/data/archive/ray/course/intro2ray/.venv/lib/python3.12/site-packages/ray/data/_internal/arrow_ops/transform_pyarrow.py\", line 546, in concat\n",
      "    table = pyarrow.Table.from_arrays(cols, schema=schema)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"pyarrow/table.pxi\", line 4868, in pyarrow.lib.Table.from_arrays\n",
      "  File \"pyarrow/table.pxi\", line 4214, in pyarrow.lib.Table.validate\n",
      "  File \"pyarrow/error.pxi\", line 92, in pyarrow.lib.check_status\n",
      "pyarrow.lib.ArrowInvalid: Column 0: In chunk 1 expected type extension<ray.data.arrow_tensor_v2<ArrowTensorTypeV2>> but saw extension<ray.data.arrow_tensor_v2<ArrowTensorTypeV2>>\n"
     ]
    }
   ],
   "source": [
    "normalized_batch = ds_normalized.take_batch(batch_size=10)\n",
    "\n",
    "for image in normalized_batch[\"image\"]:\n",
    "    assert image.shape == (1, 28, 28) # channel, height, width\n",
    "    assert image.min() >= -1 and image.max() <= 1 # normalized to [-1, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "### Activity: Add the ground truth label using the image path.\n",
    "\n",
    "In this activity, you will add the ground truth label using the image path.\n",
    "\n",
    "The image path is in the format of `s3://anyscale-public-materials/ray-ai-libraries/mnist/50_per_index/{label}/{image_id}.png`.\n",
    "\n",
    "See the suggested code below:\n",
    "\n",
    "```python\n",
    "# Hint: define the add_label function\n",
    "\n",
    "ds_labeled = ds_normalized.map_batches(add_label)\n",
    "labeled_batch = ds_labeled.take_batch(10)\n",
    "print(labeled_batch[\"ground_truth_label\"])\n",
    "```\n",
    "\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "<details>\n",
    "\n",
    "<summary>Click to view solution</summary>\n",
    "\n",
    "```python\n",
    "def add_label(batch: dict[str, np.ndarray]) -> dict[str, np.ndarray]:\n",
    "    batch[\"ground_truth_label\"] = [int(path.split(\"/\")[-2]) for path in batch[\"path\"]]\n",
    "    return batch\n",
    "\n",
    "ds_labeled = ds_normalized.map_batches(add_label)\n",
    "labeled_batch = ds_labeled.take_batch(10)\n",
    "print(labeled_batch[\"ground_truth_label\"])\n",
    "```\n",
    "\n",
    "</details>  \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stateful transformations with actors\n",
    "\n",
    "In cases like batch inference, you want to spin up a number of actor processes that are initialized once with your model and reused to process multiple batches.\n",
    "\n",
    "To implement this, you can use the `map_batches` API with a \"Callable\" class method that implements:\n",
    "\n",
    "- `__init__`: Initialize any expensive state.\n",
    "- `__call__`: Perform the stateful transformation.\n",
    "\n",
    "For example, we can implement a `MNISTClassifier` that:\n",
    "- loads a pre-trained model from a local file\n",
    "- accepts a batch of images and generates the predicted label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MNISTClassifier:\n",
    "    def __init__(self, local_path: str):\n",
    "        self.model = torch.jit.load(local_path)\n",
    "        self.model.to(\"cuda\")\n",
    "        self.model.eval()\n",
    "\n",
    "    def __call__(self, batch: dict[str, np.ndarray]) -> dict[str, np.ndarray]:\n",
    "        images = torch.tensor(batch[\"image\"]).float().to(\"cuda\")\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = self.model(images).cpu().numpy()\n",
    "\n",
    "        batch[\"predicted_label\"] = np.argmax(logits, axis=1)\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We download the model from s3 to an EFS storage\n",
    "!aws s3 cp s3://anyscale-public-materials/ray-ai-libraries/mnist/model/model.pt /mnt/cluster_storage/model.pt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use the `map_batches` API to apply the transformation to each batch of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_preds = ds_normalized.map_batches(\n",
    "    MNISTClassifier,\n",
    "    fn_constructor_kwargs={\"local_path\": \"/mnt/cluster_storage/model.pt\"},\n",
    "    num_gpus=0.1,\n",
    "    concurrency=1,\n",
    "    batch_size=100,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "<b>Note:</b> We pass in the Callable class uninitialized. Ray will pass in the arguments to the class constructor when the class is actually used in a transformation.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_preds = ds_preds.take_batch(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Materializing Data\n",
    "\n",
    "You can choose to materialize the entire dataset into the ray object store which is distributed across the cluster, primarily in memory and secondarily spilling to disk.\n",
    "\n",
    "To materialize the dataset, we can use the `materialize()` method.\n",
    "\n",
    "Use this **only** when you require the full dataset to compute downstream outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_preds.materialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Operations: Grouping, Aggregation, and Shuffling\n",
    "\n",
    "Let's look at some more involved transformations.\n",
    "\n",
    "#### Custom batching using `groupby`. \n",
    "\n",
    "In case you want to generate batches according to a specific key, you can use `groupby` to group the data by the key and then use `map_groups` to apply the transformation.\n",
    "\n",
    "For instance, let's compute the accuracy of the model by \"ground truth label\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_label(batch: dict[str, np.ndarray]) -> dict[str, np.ndarray]:\n",
    "    batch[\"ground_truth_label\"] = [int(path.split(\"/\")[-2]) for path in batch[\"path\"]]\n",
    "    return batch\n",
    "\n",
    "\n",
    "def compute_accuracy(group: dict[str, np.ndarray]) -> dict[str, np.ndarray]:\n",
    "    return {\n",
    "        \"accuracy\": [np.mean(group[\"predicted_label\"] == group[\"ground_truth_label\"])],\n",
    "        \"ground_truth_label\": group[\"ground_truth_label\"][:1],\n",
    "    }\n",
    "\n",
    "\n",
    "ds_preds.map_batches(add_label).groupby(\"ground_truth_label\").map_groups(compute_accuracy).to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "<b>Note:</b> ds_preds is not re-computed given we have already materialized the dataset.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregations\n",
    "\n",
    "Ray Data also supports a variety of aggregations. For instance, we can compute the mean accuracy across the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_preds.map_batches(add_label).map_batches(compute_accuracy).mean(on=\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As of version 2.34.0, Ray Data provides the following aggregation functions:\n",
    "\n",
    "- `count`\n",
    "- `max`\n",
    "- `mean`\n",
    "- `min`\n",
    "- `sum`\n",
    "- `std`\n",
    "\n",
    "See relevant [docs page here](https://docs.ray.io/en/latest/data/api/grouped_data.html#ray.data.aggregate.AggregateFn)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shuffling data \n",
    "\n",
    "There are different options to shuffle data in Ray Data of varying degrees of randomness and performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### File based shuffle on read\n",
    "\n",
    "To randomly shuffle the ordering of input files before reading, call a read function that supports shuffling, such as `read_images()`, and use the shuffle=\"files\" parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-15 23:07:21,563\tINFO streaming_executor.py:108 -- Starting execution of Dataset. Full logs are in /tmp/ray/session_2025-04-15_21-36-25_446080_3579077/logs/ray-data\n",
      "2025-04-15 23:07:21,565\tINFO streaming_executor.py:109 -- Execution plan of Dataset: InputDataBuffer[Input] -> TaskPoolMapOperator[ReadImage]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "829a5a213781476c9a17ba537519048e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running 0: 0.00 row [00:00, ? row/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77837a952a984abd829005131771f137",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "- ReadImage 1: 0.00 row [00:00, ? row/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-15 23:07:23,110\tINFO streaming_executor.py:108 -- Starting execution of Dataset. Full logs are in /tmp/ray/session_2025-04-15_21-36-25_446080_3579077/logs/ray-data\n",
      "2025-04-15 23:07:23,111\tINFO streaming_executor.py:109 -- Execution plan of Dataset: InputDataBuffer[Input] -> TaskPoolMapOperator[ReadImage]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21fa74f676d94577a4e974d71bced068",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running 0: 0.00 row [00:00, ? row/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "740a7111ec0847f0a933de81f77de3e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "- ReadImage 1: 0.00 row [00:00, ? row/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6329478d45074b39b21d65e6ae8256ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dataset(\n",
       "   num_rows=500,\n",
       "   schema={image: numpy.ndarray(shape=(28, 28), dtype=uint8)}\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ray.data.read_images(\"s3://anyscale-public-materials/ray-ai-libraries/mnist/50_per_index/\", shuffle=\"files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shuffling block order\n",
    "This option randomizes the order of blocks in a dataset. Blocks are the basic unit of data chunk that Ray Data stores in the object store. Applying this operation alone doesn’t involve heavy computation and communication. However, it requires Ray Data to materialize all blocks in memory before applying the operation. Only use this option when your dataset is small enough to fit into the object store memory.\n",
    "\n",
    "To perform block order shuffling, use `randomize_block_order`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ds_preds' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m ds_randomized_blocks = \u001b[43mds_preds\u001b[49m.randomize_block_order()\n\u001b[32m      2\u001b[39m ds_randomized_blocks.materialize()\n",
      "\u001b[31mNameError\u001b[39m: name 'ds_preds' is not defined"
     ]
    }
   ],
   "source": [
    "ds_randomized_blocks = ds_preds.randomize_block_order()\n",
    "ds_randomized_blocks.materialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shuffle all rows globally\n",
    "To randomly shuffle all rows globally, call `random_shuffle()`. This is the slowest option for shuffle, and requires transferring data across network between workers. This option achieves the best randomness among all options.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_randomized_rows = ds_preds.random_shuffle()\n",
    "ds_randomized_rows.materialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Persisting Data\n",
    "\n",
    "Finally, you can persist a dataset to storage using any of the \"write\" functions that Ray Data supports.\n",
    "\n",
    "Lets write our predictions to a parquet dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_preds.write_parquet(\"/mnt/cluster_storage/mnist_preds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refer to the [Input/Output docs](https://docs.ray.io/en/latest/data/api/input_output.html) for a comprehensive list of write functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleanup\n",
    "!rm -rf /mnt/cluster_storage/mnist_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Ray Data in Production\n",
    "\n",
    "1. Runway AI is using Ray Data to scale its ML workloads. See [this interview with Runway AI](https://siliconangle.com/2024/10/02/runway-transforming-ai-driven-filmmaking-innovative-tools-techniques-raysummit/) to learn more.\n",
    "2. Netflix is using Ray Data for multi-modal inference pipelines. See [this talk at the Ray Summit 2024](https://raysummit.anyscale.com/flow/anyscale/raysummit2024/landing/page/sessioncatalog/session/1722028596844001bCg0) to learn more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
