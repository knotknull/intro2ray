{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "682e224e-1bb9-470c-b363-386ede0785a4",
   "metadata": {},
   "source": [
    "# Intro to Ray Serve\n",
    "\n",
    "This notebook will introduce you to Ray Serve, a framework for building and deploying scalable ML applications.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<b>Here is the roadmap for this notebook:</b>\n",
    "\n",
    "<ul>\n",
    "    <li><b>Part 1:</b> Overview of Ray Serve</li>\n",
    "    <li><b>Part 2:</b> Implement an MNISTClassifier service</li>\n",
    "    <li><b>Part 3:</b> Advanced features of Ray Serve</li>\n",
    "    <li><b>Part 4:</b> Ray Serve in Production</li>\n",
    "</ul>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1060aea0",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "099b7710",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "from torchvision import transforms\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import ray\n",
    "import requests\n",
    "import torch\n",
    "from ray import serve\n",
    "from matplotlib import pyplot as plt\n",
    "from fastapi import FastAPI\n",
    "from starlette.requests import Request"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7250cc03-e52c-4e30-a262-8d8e0a5a0837",
   "metadata": {},
   "source": [
    "## 1. Overview of Ray Serve\n",
    "\n",
    "Serve is a framework for serving ML applications. \n",
    "\n",
    "Here is a high-level overview of the architecture of a Ray Serve Application.\n",
    "\n",
    "<img src='https://technical-training-assets.s3.us-west-2.amazonaws.com/Ray_Serve/serve_architecture.png' width=700/>\n",
    "\n",
    "An Application is a collection of one or more Deployments that are deployed together.\n",
    "\n",
    "### Deployments\n",
    "\n",
    "`Deployment` is the fundamental developer-facing element of serve.\n",
    "\n",
    "<img src='https://technical-training-assets.s3.us-west-2.amazonaws.com/Ray_Serve/deployment.png' width=600/>\n",
    "\n",
    "Each deployment can have multiple replicas. \n",
    "\n",
    "A replica is implemented as a Ray actor with a queue to process incoming requests.\n",
    "\n",
    "Each replica can be configured with a set of compute resources. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6380b141",
   "metadata": {},
   "source": [
    "### When to use Ray Serve?\n",
    "\n",
    "Ray Serve is designed to be used in the following scenarios:\n",
    "- Build end-to-end ML applications with a flexible and programmable python API\n",
    "- Flexibly scale up and down your compute resources to meet the demand of your application\n",
    "- Easy to develop on a local machine, and scale to a multi-node GPU cluster\n",
    "\n",
    "#### Key Ray Serve Features\n",
    "Ray Serve provides the following key features and optimizations:\n",
    "- [response streaming](https://docs.ray.io/en/latest/serve/tutorials/streaming.html)\n",
    "- [dynamic request batching](https://docs.ray.io/en/latest/serve/advanced-guides/dyn-req-batch.html)\n",
    "- [multi-node/multi-GPU serving](https://docs.ray.io/en/latest/serve/tutorials/vllm-example.html)\n",
    "- [model multiplexing](https://docs.ray.io/en/latest/serve/model-multiplexing.html)\n",
    "- [fractional compute resource usage](https://docs.ray.io/en/latest/serve/configure-serve-deployment.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43da1a6",
   "metadata": {},
   "source": [
    "## 2. Implement an MNISTClassifier service\n",
    "\n",
    "Letâ€™s jump right in and get a simple ML service up and running on Ray Serve. \n",
    "\n",
    "Recall the `MNISTClassifier` we built to perform batch inference on the `MNIST` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14fb17a6-a71c-4a11-8ea8-b1b350a5fa1c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class OfflineMNISTClassifier:\n",
    "    def __init__(self, local_path: str):\n",
    "        self.model = torch.jit.load(local_path)\n",
    "        self.model.to(\"cuda\")\n",
    "        self.model.eval()\n",
    "\n",
    "    def __call__(self, batch: dict[str, np.ndarray]) -> dict[str, np.ndarray]:\n",
    "        return self.predict(batch)\n",
    "    \n",
    "    def predict(self, batch: dict[str, np.ndarray]) -> dict[str, np.ndarray]:\n",
    "        images = torch.tensor(batch[\"image\"]).float().to(\"cuda\")\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = self.model(images).cpu().numpy()\n",
    "\n",
    "        batch[\"predicted_label\"] = np.argmax(logits, axis=1)\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48d148b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://anyscale-public-materials/ray-ai-libraries/mnist/model/model.pt to ../../../../../../mnt/cluster_storage/model.pt\n"
     ]
    }
   ],
   "source": [
    "# We download the model from s3 to the EFS storage\n",
    "!aws s3 cp s3://anyscale-public-materials/ray-ai-libraries/mnist/model/model.pt /mnt/cluster_storage/model.pt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a79961",
   "metadata": {},
   "source": [
    "Here is how we can use the `OfflineMNISTClassifier` to perform batch inference on a dataset of random images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41b16400",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-16 14:22:03,039\tINFO worker.py:1843 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "2025-04-16 14:22:04,885\tINFO streaming_executor.py:108 -- Starting execution of Dataset. Full logs are in /tmp/ray/session_2025-04-16_14-22-01_744497_3812578/logs/ray-data\n",
      "2025-04-16 14:22:04,886\tINFO streaming_executor.py:109 -- Execution plan of Dataset: InputDataBuffer[Input] -> ActorPoolMapOperator[MapBatches(OfflineMNISTClassifier)] -> LimitOperator[limit=10]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b6ffb99892447a1b4f1aae39bf7e754",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running 0: 0.00 row [00:00, ? row/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17155f1528f94e1797462ac49be6fe7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "- MapBatches(OfflineMNISTClassifier) 1: 0.00 row [00:00, ? row/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8463f4bc103f4d97913d8d46d785e3f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "- limit=10 2: 0.00 row [00:00, ? row/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([6, 1, 6, 6, 6, 1, 6, 6, 6, 1])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ProxyActor pid=4005504)\u001b[0m INFO 2025-04-16 14:23:25,716 proxy 192.168.99.98 -- Proxy starting on node b5c3f964f3685e6e9f297622e3fe2b306edecca5177c480a37ad9a77 (HTTP port: 8000).\n",
      "\u001b[36m(ProxyActor pid=4005504)\u001b[0m INFO 2025-04-16 14:23:25,772 proxy 192.168.99.98 -- Got updated endpoints: {}.\n",
      "\u001b[36m(ServeController pid=4005503)\u001b[0m INFO 2025-04-16 14:23:26,037 controller 4005503 -- Deploying new version of Deployment(name='OnlineMNISTClassifier', app='mnist_classifier') (initial target replicas: 1).\n",
      "\u001b[36m(ProxyActor pid=4005504)\u001b[0m INFO 2025-04-16 14:23:26,040 proxy 192.168.99.98 -- Got updated endpoints: {Deployment(name='OnlineMNISTClassifier', app='mnist_classifier'): EndpointInfo(route='/', app_is_cross_language=False)}.\n",
      "\u001b[36m(ProxyActor pid=4005504)\u001b[0m INFO 2025-04-16 14:23:26,048 proxy 192.168.99.98 -- Started <ray.serve._private.router.SharedRouterLongPollClient object at 0x714efff45d90>.\n",
      "\u001b[36m(ServeController pid=4005503)\u001b[0m INFO 2025-04-16 14:23:26,140 controller 4005503 -- Adding 1 replica to Deployment(name='OnlineMNISTClassifier', app='mnist_classifier').\n",
      "\u001b[36m(ServeReplica:mnist_classifier:OnlineMNISTClassifier pid=4005501)\u001b[0m INFO 2025-04-16 14:23:36,467 mnist_classifier_OnlineMNISTClassifier 86jmu7jw 315c3f80-3435-4ab8-a9bd-278488239ae5 -- POST / 200 346.6ms\n",
      "\u001b[36m(ServeReplica:mnist_classifier:OnlineMNISTClassifier pid=4005501)\u001b[0m INFO 2025-04-16 14:23:45,777 mnist_classifier_OnlineMNISTClassifier 86jmu7jw 14d01059-d32b-40b4-915f-2757ecd8a15a -- CALL predict OK 100.7ms\n",
      "\u001b[36m(ServeController pid=4005503)\u001b[0m INFO 2025-04-16 14:24:00,348 controller 4005503 -- Deploying new version of Deployment(name='OnlineMNISTClassifier', app='mnist_classifier') (initial target replicas: 4).\n",
      "\u001b[36m(ServeController pid=4005503)\u001b[0m INFO 2025-04-16 14:24:00,452 controller 4005503 -- Adding 3 replicas to Deployment(name='OnlineMNISTClassifier', app='mnist_classifier').\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(autoscaler +3m19s)\u001b[0m Tip: use `ray status` to view detailed cluster status. To disable these messages, set RAY_SCHEDULER_EVENTS=0.\n",
      "\u001b[33m(autoscaler +3m19s)\u001b[0m Warning: The following resource request cannot be scheduled right now: {'GPU': 0.1, 'CPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ServeController pid=4005503)\u001b[0m WARNING 2025-04-16 14:24:30,460 controller 4005503 -- Deployment 'OnlineMNISTClassifier' in application 'mnist_classifier' has 3 replicas that have taken more than 30s to be scheduled. This may be due to waiting for the cluster to auto-scale or for a runtime environment to be installed. Resources required for each replica: {\"CPU\": 1, \"GPU\": 0.1}, total resources available: {\"CPU\": 7.0}. Use `ray status` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m(autoscaler +3m54s)\u001b[0m Warning: The following resource request cannot be scheduled right now: {'GPU': 0.1, 'CPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ServeController pid=4005503)\u001b[0m WARNING 2025-04-16 14:25:00,482 controller 4005503 -- Deployment 'OnlineMNISTClassifier' in application 'mnist_classifier' has 3 replicas that have taken more than 30s to be scheduled. This may be due to waiting for the cluster to auto-scale or for a runtime environment to be installed. Resources required for each replica: {\"CPU\": 1, \"GPU\": 0.1}, total resources available: {\"CPU\": 7.0}. Use `ray status` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m(autoscaler +4m29s)\u001b[0m Warning: The following resource request cannot be scheduled right now: {'GPU': 0.1, 'CPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ServeController pid=4005503)\u001b[0m WARNING 2025-04-16 14:25:30,577 controller 4005503 -- Deployment 'OnlineMNISTClassifier' in application 'mnist_classifier' has 3 replicas that have taken more than 30s to be scheduled. This may be due to waiting for the cluster to auto-scale or for a runtime environment to be installed. Resources required for each replica: {\"CPU\": 1, \"GPU\": 0.1}, total resources available: {\"CPU\": 7.0}. Use `ray status` for more details.\n",
      "\u001b[36m(ServeController pid=4005503)\u001b[0m WARNING 2025-04-16 14:26:00,658 controller 4005503 -- Deployment 'OnlineMNISTClassifier' in application 'mnist_classifier' has 3 replicas that have taken more than 30s to be scheduled. This may be due to waiting for the cluster to auto-scale or for a runtime environment to be installed. Resources required for each replica: {\"CPU\": 1, \"GPU\": 0.1}, total resources available: {\"CPU\": 7.0}. Use `ray status` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m(autoscaler +5m4s)\u001b[0m Warning: The following resource request cannot be scheduled right now: {'GPU': 0.1, 'CPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ServeController pid=4005503)\u001b[0m WARNING 2025-04-16 14:26:30,669 controller 4005503 -- Deployment 'OnlineMNISTClassifier' in application 'mnist_classifier' has 3 replicas that have taken more than 30s to be scheduled. This may be due to waiting for the cluster to auto-scale or for a runtime environment to be installed. Resources required for each replica: {\"CPU\": 1, \"GPU\": 0.1}, total resources available: {\"CPU\": 7.0}. Use `ray status` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m(autoscaler +5m39s)\u001b[0m Warning: The following resource request cannot be scheduled right now: {'GPU': 0.1, 'CPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ServeController pid=4005503)\u001b[0m WARNING 2025-04-16 14:27:00,681 controller 4005503 -- Deployment 'OnlineMNISTClassifier' in application 'mnist_classifier' has 3 replicas that have taken more than 30s to be scheduled. This may be due to waiting for the cluster to auto-scale or for a runtime environment to be installed. Resources required for each replica: {\"CPU\": 1, \"GPU\": 0.1}, total resources available: {\"CPU\": 7.0}. Use `ray status` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m(autoscaler +6m15s)\u001b[0m Warning: The following resource request cannot be scheduled right now: {'GPU': 0.1, 'CPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ServeController pid=4005503)\u001b[0m WARNING 2025-04-16 14:27:30,702 controller 4005503 -- Deployment 'OnlineMNISTClassifier' in application 'mnist_classifier' has 3 replicas that have taken more than 30s to be scheduled. This may be due to waiting for the cluster to auto-scale or for a runtime environment to be installed. Resources required for each replica: {\"CPU\": 1, \"GPU\": 0.1}, total resources available: {\"CPU\": 7.0}. Use `ray status` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m(autoscaler +6m50s)\u001b[0m Warning: The following resource request cannot be scheduled right now: {'GPU': 0.1, 'CPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ServeController pid=4005503)\u001b[0m WARNING 2025-04-16 14:28:00,762 controller 4005503 -- Deployment 'OnlineMNISTClassifier' in application 'mnist_classifier' has 3 replicas that have taken more than 30s to be scheduled. This may be due to waiting for the cluster to auto-scale or for a runtime environment to be installed. Resources required for each replica: {\"CPU\": 1, \"GPU\": 0.1}, total resources available: {\"CPU\": 7.0}. Use `ray status` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m(autoscaler +7m25s)\u001b[0m Warning: The following resource request cannot be scheduled right now: {'GPU': 0.1, 'CPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ServeController pid=4005503)\u001b[0m WARNING 2025-04-16 14:28:30,861 controller 4005503 -- Deployment 'OnlineMNISTClassifier' in application 'mnist_classifier' has 3 replicas that have taken more than 30s to be scheduled. This may be due to waiting for the cluster to auto-scale or for a runtime environment to be installed. Resources required for each replica: {\"CPU\": 1, \"GPU\": 0.1}, total resources available: {\"CPU\": 7.0}. Use `ray status` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m(autoscaler +8m0s)\u001b[0m Warning: The following resource request cannot be scheduled right now: {'GPU': 0.1, 'CPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ServeController pid=4005503)\u001b[0m WARNING 2025-04-16 14:29:00,969 controller 4005503 -- Deployment 'OnlineMNISTClassifier' in application 'mnist_classifier' has 3 replicas that have taken more than 30s to be scheduled. This may be due to waiting for the cluster to auto-scale or for a runtime environment to be installed. Resources required for each replica: {\"CPU\": 1, \"GPU\": 0.1}, total resources available: {\"CPU\": 7.0}. Use `ray status` for more details.\n",
      "\u001b[36m(ServeController pid=4005503)\u001b[0m WARNING 2025-04-16 14:29:30,985 controller 4005503 -- Deployment 'OnlineMNISTClassifier' in application 'mnist_classifier' has 3 replicas that have taken more than 30s to be scheduled. This may be due to waiting for the cluster to auto-scale or for a runtime environment to be installed. Resources required for each replica: {\"CPU\": 1, \"GPU\": 0.1}, total resources available: {\"CPU\": 7.0}. Use `ray status` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m(autoscaler +8m35s)\u001b[0m Warning: The following resource request cannot be scheduled right now: {'GPU': 0.1, 'CPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ServeController pid=4005503)\u001b[0m WARNING 2025-04-16 14:30:01,047 controller 4005503 -- Deployment 'OnlineMNISTClassifier' in application 'mnist_classifier' has 3 replicas that have taken more than 30s to be scheduled. This may be due to waiting for the cluster to auto-scale or for a runtime environment to be installed. Resources required for each replica: {\"CPU\": 1, \"GPU\": 0.1}, total resources available: {\"CPU\": 7.0}. Use `ray status` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m(autoscaler +9m10s)\u001b[0m Warning: The following resource request cannot be scheduled right now: {'GPU': 0.1, 'CPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ServeController pid=4005503)\u001b[0m WARNING 2025-04-16 14:30:31,117 controller 4005503 -- Deployment 'OnlineMNISTClassifier' in application 'mnist_classifier' has 3 replicas that have taken more than 30s to be scheduled. This may be due to waiting for the cluster to auto-scale or for a runtime environment to be installed. Resources required for each replica: {\"CPU\": 1, \"GPU\": 0.1}, total resources available: {\"CPU\": 7.0}. Use `ray status` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m(autoscaler +9m45s)\u001b[0m Warning: The following resource request cannot be scheduled right now: {'GPU': 0.1, 'CPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ServeController pid=4005503)\u001b[0m WARNING 2025-04-16 14:31:01,187 controller 4005503 -- Deployment 'OnlineMNISTClassifier' in application 'mnist_classifier' has 3 replicas that have taken more than 30s to be scheduled. This may be due to waiting for the cluster to auto-scale or for a runtime environment to be installed. Resources required for each replica: {\"CPU\": 1, \"GPU\": 0.1}, total resources available: {\"CPU\": 7.0}. Use `ray status` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m(autoscaler +10m20s)\u001b[0m Warning: The following resource request cannot be scheduled right now: {'GPU': 0.1, 'CPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ServeController pid=4005503)\u001b[0m WARNING 2025-04-16 14:31:31,292 controller 4005503 -- Deployment 'OnlineMNISTClassifier' in application 'mnist_classifier' has 3 replicas that have taken more than 30s to be scheduled. This may be due to waiting for the cluster to auto-scale or for a runtime environment to be installed. Resources required for each replica: {\"CPU\": 1, \"GPU\": 0.1}, total resources available: {\"CPU\": 7.0}. Use `ray status` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m(autoscaler +10m55s)\u001b[0m Warning: The following resource request cannot be scheduled right now: {'GPU': 0.1, 'CPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ServeController pid=4005503)\u001b[0m WARNING 2025-04-16 14:32:01,320 controller 4005503 -- Deployment 'OnlineMNISTClassifier' in application 'mnist_classifier' has 3 replicas that have taken more than 30s to be scheduled. This may be due to waiting for the cluster to auto-scale or for a runtime environment to be installed. Resources required for each replica: {\"CPU\": 1, \"GPU\": 0.1}, total resources available: {\"CPU\": 7.0}. Use `ray status` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m(autoscaler +11m30s)\u001b[0m Warning: The following resource request cannot be scheduled right now: {'GPU': 0.1, 'CPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ServeController pid=4005503)\u001b[0m WARNING 2025-04-16 14:32:31,397 controller 4005503 -- Deployment 'OnlineMNISTClassifier' in application 'mnist_classifier' has 3 replicas that have taken more than 30s to be scheduled. This may be due to waiting for the cluster to auto-scale or for a runtime environment to be installed. Resources required for each replica: {\"CPU\": 1, \"GPU\": 0.1}, total resources available: {\"CPU\": 7.0}. Use `ray status` for more details.\n",
      "\u001b[36m(ServeController pid=4005503)\u001b[0m WARNING 2025-04-16 14:33:01,443 controller 4005503 -- Deployment 'OnlineMNISTClassifier' in application 'mnist_classifier' has 3 replicas that have taken more than 30s to be scheduled. This may be due to waiting for the cluster to auto-scale or for a runtime environment to be installed. Resources required for each replica: {\"CPU\": 1, \"GPU\": 0.1}, total resources available: {\"CPU\": 7.0}. Use `ray status` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m(autoscaler +12m5s)\u001b[0m Warning: The following resource request cannot be scheduled right now: {'GPU': 0.1, 'CPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ServeController pid=4005503)\u001b[0m WARNING 2025-04-16 14:33:31,529 controller 4005503 -- Deployment 'OnlineMNISTClassifier' in application 'mnist_classifier' has 3 replicas that have taken more than 30s to be scheduled. This may be due to waiting for the cluster to auto-scale or for a runtime environment to be installed. Resources required for each replica: {\"CPU\": 1, \"GPU\": 0.1}, total resources available: {\"CPU\": 7.0}. Use `ray status` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m(autoscaler +12m40s)\u001b[0m Warning: The following resource request cannot be scheduled right now: {'GPU': 0.1, 'CPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ServeController pid=4005503)\u001b[0m WARNING 2025-04-16 14:34:01,547 controller 4005503 -- Deployment 'OnlineMNISTClassifier' in application 'mnist_classifier' has 3 replicas that have taken more than 30s to be scheduled. This may be due to waiting for the cluster to auto-scale or for a runtime environment to be installed. Resources required for each replica: {\"CPU\": 1, \"GPU\": 0.1}, total resources available: {\"CPU\": 7.0}. Use `ray status` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m(autoscaler +13m15s)\u001b[0m Warning: The following resource request cannot be scheduled right now: {'GPU': 0.1, 'CPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ServeController pid=4005503)\u001b[0m WARNING 2025-04-16 14:34:31,638 controller 4005503 -- Deployment 'OnlineMNISTClassifier' in application 'mnist_classifier' has 3 replicas that have taken more than 30s to be scheduled. This may be due to waiting for the cluster to auto-scale or for a runtime environment to be installed. Resources required for each replica: {\"CPU\": 1, \"GPU\": 0.1}, total resources available: {\"CPU\": 7.0}. Use `ray status` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m(autoscaler +13m50s)\u001b[0m Warning: The following resource request cannot be scheduled right now: {'GPU': 0.1, 'CPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ServeController pid=4005503)\u001b[0m WARNING 2025-04-16 14:35:01,646 controller 4005503 -- Deployment 'OnlineMNISTClassifier' in application 'mnist_classifier' has 3 replicas that have taken more than 30s to be scheduled. This may be due to waiting for the cluster to auto-scale or for a runtime environment to be installed. Resources required for each replica: {\"CPU\": 1, \"GPU\": 0.1}, total resources available: {\"CPU\": 7.0}. Use `ray status` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m(autoscaler +14m25s)\u001b[0m Warning: The following resource request cannot be scheduled right now: {'GPU': 0.1, 'CPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ServeController pid=4005503)\u001b[0m WARNING 2025-04-16 14:35:31,735 controller 4005503 -- Deployment 'OnlineMNISTClassifier' in application 'mnist_classifier' has 3 replicas that have taken more than 30s to be scheduled. This may be due to waiting for the cluster to auto-scale or for a runtime environment to be installed. Resources required for each replica: {\"CPU\": 1, \"GPU\": 0.1}, total resources available: {\"CPU\": 7.0}. Use `ray status` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m(autoscaler +15m0s)\u001b[0m Warning: The following resource request cannot be scheduled right now: {'GPU': 0.1, 'CPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ServeController pid=4005503)\u001b[0m WARNING 2025-04-16 14:36:01,808 controller 4005503 -- Deployment 'OnlineMNISTClassifier' in application 'mnist_classifier' has 3 replicas that have taken more than 30s to be scheduled. This may be due to waiting for the cluster to auto-scale or for a runtime environment to be installed. Resources required for each replica: {\"CPU\": 1, \"GPU\": 0.1}, total resources available: {\"CPU\": 7.0}. Use `ray status` for more details.\n"
     ]
    }
   ],
   "source": [
    "# Create a dataset of random images\n",
    "ds = ray.data.from_items([{\"image\": np.random.rand(1, 28, 28)} for _ in range(100)])\n",
    "\n",
    "# Map the OfflineMNISTClassifier to the dataset\n",
    "ds = ds.map_batches(\n",
    "    OfflineMNISTClassifier,\n",
    "    fn_constructor_kwargs={\"local_path\": \"/mnt/cluster_storage/model.pt\"},\n",
    "    concurrency=1,\n",
    "    num_gpus=1,\n",
    "    batch_size=10\n",
    ")\n",
    "\n",
    "# Take a look at the first 10 predictions\n",
    "ds.take_batch(10)[\"predicted_label\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb1a687",
   "metadata": {},
   "source": [
    "Now, if want to migrate to an online inference setting, we can transform this into a Ray Serve Deployment by applying the `@serve.deployment` decorator \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c68888dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "@serve.deployment() # this is the decorator to add\n",
    "class OnlineMNISTClassifier:\n",
    "    def __init__(self, local_path: str):\n",
    "        self.model = torch.jit.load(local_path)\n",
    "        self.model.to(\"cuda\")\n",
    "        self.model.eval()\n",
    "\n",
    "    async def __call__(self, request: Request) -> dict[str, Any]: # __call__ now takes a Starlette Request object\n",
    "        batch = json.loads(await request.json()) # we will need to parse the JSON body of the request\n",
    "        return await self.predict(batch)\n",
    "    \n",
    "    async def predict(self, batch: dict[str, np.ndarray]) -> dict[str, np.ndarray]:\n",
    "        # same code as OfflineMNISTClassifier.predict except we added async to the method\n",
    "        images = torch.tensor(batch[\"image\"]).float().to(\"cuda\")\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = self.model(images).cpu().numpy()\n",
    "\n",
    "        batch[\"predicted_label\"] = np.argmax(logits, axis=1)\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf85ff1",
   "metadata": {},
   "source": [
    "We can now instantiate the `OnlineMNISTClassifier` as a Ray Serve Application using `.bind`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df46ddd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_deployment = OnlineMNISTClassifier.options(\n",
    "    num_replicas=1,\n",
    "    ray_actor_options={\"num_gpus\": 1},\n",
    ")\n",
    "\n",
    "mnist_app = mnist_deployment.bind(local_path=\"/mnt/cluster_storage/model.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098e8ac4",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "**Note:** `.bind` is a method that takes in the arguments to pass to the Deployment constructor.\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e70529",
   "metadata": {},
   "source": [
    "We can then run the application "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e96056cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO 2025-04-16 14:23:25,991 serve 3812578 -- Started Serve in namespace \"serve\".\n",
      "INFO 2025-04-16 14:23:29,119 serve 3812578 -- Application 'mnist_classifier' is ready at http://127.0.0.1:8000/.\n"
     ]
    }
   ],
   "source": [
    "mnist_deployment_handle = serve.run(mnist_app, name='mnist_classifier', blocking=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4a0cdb-822a-4439-aeab-9916dd8d059c",
   "metadata": {},
   "source": [
    "We can test it as an HTTP endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c0a80e9-c26f-48d2-8985-ef4eab4dc580",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 6]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images = np.random.rand(2, 1, 28, 28).tolist()\n",
    "json_request = json.dumps({\"image\": images})\n",
    "response = requests.post(\"http://localhost:8000/\", json=json_request)\n",
    "response.json()[\"predicted_label\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd2cb01",
   "metadata": {},
   "source": [
    "We can also test it as a gRPC endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "342928ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO 2025-04-16 14:23:45,662 serve 3812578 -- Started <ray.serve._private.router.SharedRouterLongPollClient object at 0x7c082c1078c0>.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([6, 1, 1, 6, 6, 1, 6, 6, 6, 1])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = {\"image\": np.random.rand(10, 1, 28, 28)}\n",
    "response = await mnist_deployment_handle.predict.remote(batch)\n",
    "response[\"predicted_label\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e170084",
   "metadata": {},
   "source": [
    "## 3. Advanced features of Ray Serve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2b22a2",
   "metadata": {},
   "source": [
    "### Using fractions of a GPU\n",
    "\n",
    "With Ray we can specify fractional compute resources for each deployment's replica. \n",
    "\n",
    "This is useful to help us fully utilize a GPU especially when running small models like our `MNISTClassifier` model.\n",
    "\n",
    "Here is how to specify only 10% of a GPU's compute resources for our `MNISTClassifier` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "230f9ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mnist_app = OnlineMNISTClassifier.options(\n",
    "#    num_replicas=4, # we can scale to up to 10 replicas on a single GPU\n",
    "#    ray_actor_options={\"num_gpus\": 0.1}, \n",
    "#).bind(local_path=\"/mnt/cluster_storage/model.pt\")\n",
    "\n",
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35a8d83",
   "metadata": {},
   "source": [
    "Next we update the running application by running serve.run with the new options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9e9ad6fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO 2025-04-16 14:24:00,313 serve 3812578 -- Connecting to existing Serve app in namespace \"serve\". New http options will not be applied.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m mnist_deployment_handle = \u001b[43mserve\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmnist_app\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmnist_classifier\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblocking\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data/archive/ray/course/intro2ray/.venv/lib/python3.12/site-packages/ray/serve/api.py:625\u001b[39m, in \u001b[36mrun\u001b[39m\u001b[34m(target, blocking, name, route_prefix, logging_config, _local_testing_mode)\u001b[39m\n\u001b[32m    590\u001b[39m \u001b[38;5;129m@PublicAPI\u001b[39m(stability=\u001b[33m\"\u001b[39m\u001b[33mstable\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    591\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrun\u001b[39m(\n\u001b[32m    592\u001b[39m     target: Application,\n\u001b[32m   (...)\u001b[39m\u001b[32m    597\u001b[39m     _local_testing_mode: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    598\u001b[39m ) -> DeploymentHandle:\n\u001b[32m    599\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Run an application and return a handle to its ingress deployment.\u001b[39;00m\n\u001b[32m    600\u001b[39m \n\u001b[32m    601\u001b[39m \u001b[33;03m    The application is returned by `Deployment.bind()`. Example:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    623\u001b[39m \u001b[33;03m        DeploymentHandle: A handle that can be used to call the application.\u001b[39;00m\n\u001b[32m    624\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m625\u001b[39m     handle = \u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    626\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    628\u001b[39m \u001b[43m        \u001b[49m\u001b[43mroute_prefix\u001b[49m\u001b[43m=\u001b[49m\u001b[43mroute_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    629\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlogging_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlogging_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    630\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_local_testing_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_local_testing_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    631\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    633\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m blocking:\n\u001b[32m    634\u001b[39m         wait_for_interrupt()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data/archive/ray/course/intro2ray/.venv/lib/python3.12/site-packages/ray/serve/api.py:535\u001b[39m, in \u001b[36m_run\u001b[39m\u001b[34m(target, _blocking, name, route_prefix, logging_config, _local_testing_mode)\u001b[39m\n\u001b[32m    520\u001b[39m \u001b[38;5;129m@PublicAPI\u001b[39m(stability=\u001b[33m\"\u001b[39m\u001b[33mstable\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    521\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_run\u001b[39m(\n\u001b[32m    522\u001b[39m     target: Application,\n\u001b[32m   (...)\u001b[39m\u001b[32m    528\u001b[39m     _local_testing_mode: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    529\u001b[39m ) -> DeploymentHandle:\n\u001b[32m    530\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Run an application and return a handle to its ingress deployment.\u001b[39;00m\n\u001b[32m    531\u001b[39m \n\u001b[32m    532\u001b[39m \u001b[33;03m    This is only used internally with the _blocking not totally blocking the following\u001b[39;00m\n\u001b[32m    533\u001b[39m \u001b[33;03m    code indefinitely until Ctrl-C'd.\u001b[39;00m\n\u001b[32m    534\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m535\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_run_many\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    536\u001b[39m \u001b[43m        \u001b[49m\u001b[43m[\u001b[49m\n\u001b[32m    537\u001b[39m \u001b[43m            \u001b[49m\u001b[43mRunTarget\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    538\u001b[39m \u001b[43m                \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    539\u001b[39m \u001b[43m                \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    540\u001b[39m \u001b[43m                \u001b[49m\u001b[43mroute_prefix\u001b[49m\u001b[43m=\u001b[49m\u001b[43mroute_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    541\u001b[39m \u001b[43m                \u001b[49m\u001b[43mlogging_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlogging_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    542\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    543\u001b[39m \u001b[43m        \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    544\u001b[39m \u001b[43m        \u001b[49m\u001b[43mwait_for_applications_running\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    545\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_local_testing_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_local_testing_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    546\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data/archive/ray/course/intro2ray/.venv/lib/python3.12/site-packages/ray/serve/api.py:513\u001b[39m, in \u001b[36m_run_many\u001b[39m\u001b[34m(targets, wait_for_ingress_deployment_creation, wait_for_applications_running, _local_testing_mode)\u001b[39m\n\u001b[32m    510\u001b[39m \u001b[38;5;66;03m# Record after Ray has been started.\u001b[39;00m\n\u001b[32m    511\u001b[39m ServeUsageTag.API_VERSION.record(\u001b[33m\"\u001b[39m\u001b[33mv2\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m513\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdeploy_applications\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    514\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbuilt_apps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    515\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwait_for_ingress_deployment_creation\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwait_for_ingress_deployment_creation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    516\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwait_for_applications_running\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwait_for_applications_running\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    517\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data/archive/ray/course/intro2ray/.venv/lib/python3.12/site-packages/ray/serve/_private/client.py:52\u001b[39m, in \u001b[36m_ensure_connected.<locals>.check\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     50\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._shutdown:\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m RayServeException(\u001b[33m\"\u001b[39m\u001b[33mClient has already been shut down.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data/archive/ray/course/intro2ray/.venv/lib/python3.12/site-packages/ray/serve/_private/client.py:311\u001b[39m, in \u001b[36mServeControllerClient.deploy_applications\u001b[39m\u001b[34m(self, built_apps, wait_for_ingress_deployment_creation, wait_for_applications_running)\u001b[39m\n\u001b[32m    308\u001b[39m     \u001b[38;5;28mself\u001b[39m._wait_for_deployment_created(app.ingress_deployment_name, app.name)\n\u001b[32m    310\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m wait_for_applications_running:\n\u001b[32m--> \u001b[39m\u001b[32m311\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_wait_for_application_running\u001b[49m\u001b[43m(\u001b[49m\u001b[43mapp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    312\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m app.route_prefix \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    313\u001b[39m         url_part = \u001b[33m\"\u001b[39m\u001b[33m at \u001b[39m\u001b[33m\"\u001b[39m + \u001b[38;5;28mself\u001b[39m._root_url + app.route_prefix\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data/archive/ray/course/intro2ray/.venv/lib/python3.12/site-packages/ray/serve/_private/client.py:243\u001b[39m, in \u001b[36mServeControllerClient._wait_for_application_running\u001b[39m\u001b[34m(self, name, timeout_s)\u001b[39m\n\u001b[32m    235\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    236\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDeploying application \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m failed: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstatus.app_status.message\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    237\u001b[39m         )\n\u001b[32m    239\u001b[39m     logger.debug(\n\u001b[32m    240\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mWaiting for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m to be RUNNING, current status: \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    241\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstatus.app_status.status\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    242\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m243\u001b[39m     \u001b[43mtime\u001b[49m\u001b[43m.\u001b[49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCLIENT_POLLING_INTERVAL_S\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    244\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    245\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m(\n\u001b[32m    246\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mApplication \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m did not become RUNNING after \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtimeout_s\u001b[38;5;132;01m}\u001b[39;00m\u001b[33ms.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    247\u001b[39m     )\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "mnist_deployment_handle = serve.run(mnist_app, name='mnist_classifier', blocking=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b196a535",
   "metadata": {},
   "source": [
    "We can test the new application by sending a sample request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5aad97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = np.random.rand(2, 1, 28, 28).tolist()\n",
    "json_request = json.dumps({\"image\": images})\n",
    "response = requests.post(\"http://localhost:8000/\", json=json_request)\n",
    "response.json()[\"predicted_label\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05041234",
   "metadata": {},
   "source": [
    "### Customizing autoscaling\n",
    "\n",
    "Ray Serve provides a simple way to autoscale the number of replicas in a deployment. It is primarily based on the target number of ongoing requests per replica.\n",
    "\n",
    "i.e. here is how we can set the autoscaling config for our `OnlineMNISTClassifier` deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e356f749",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_app = OnlineMNISTClassifier.options(\n",
    "    ray_actor_options={\"num_gpus\": 0.1}, \n",
    "    autoscaling_config={\n",
    "        \"target_ongoing_requests\": 10,\n",
    "    },\n",
    ").bind(local_path=\"/mnt/cluster_storage/model.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae8a244",
   "metadata": {},
   "source": [
    "We can also control more granularly the autoscaling logic by setting:\n",
    "- the upscale and downscale delays\n",
    "- the intervals at which the replica sends metrics reports about the current number of ongoing requests\n",
    "- the look-back period used to evaluate the current number of ongoing requests\n",
    "\n",
    "Here is an example of how to set these options:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5594d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_app = OnlineMNISTClassifier.options(\n",
    "    ray_actor_options={\"num_gpus\": 0.1}, \n",
    "    autoscaling_config={\n",
    "        \"target_ongoing_requests\": 10,\n",
    "        \"upscale_delay_s\": 10,\n",
    "        \"downscale_delay_s\": 10,\n",
    "        \"metrics_interval_s\": 10,\n",
    "        \"look_back_period_s\": 10, \n",
    "    },\n",
    ").bind(local_path=\"/mnt/cluster_storage/model.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a643b4",
   "metadata": {},
   "source": [
    "We can additionally control the minimum and maximum number of replicas that can be scaled up and down. \n",
    "\n",
    "We can even specify to start scaling up from 0 replicas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebea6c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_app = OnlineMNISTClassifier.options(\n",
    "    ray_actor_options={\"num_gpus\": 0.1}, \n",
    "    autoscaling_config={\n",
    "        \"target_ongoing_requests\": 10,\n",
    "        \"initial_replicas\": 0, # scale up from 0 replicas\n",
    "        \"min_replicas\": 0,\n",
    "        \"max_replicas\": 10,\n",
    "        # extreme upscale speeds\n",
    "        \"upscale_delay_s\": 0,\n",
    "        \"metrics_interval_s\": 0.1,\n",
    "        \"look_back_period_s\": 0.1,\n",
    "    },\n",
    ").bind(local_path=\"/mnt/cluster_storage/model.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e040d6ac",
   "metadata": {},
   "source": [
    "Let's run the application with the new autoscaling config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe684a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_deployment_handle = serve.run(mnist_app, name='mnist_classifier', blocking=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75be6e25",
   "metadata": {},
   "source": [
    "Looking at the Ray Serve dashboard, we can see we are currently at 0 replicas - i.e. no GPU resources are being used.\n",
    "\n",
    "<img src='https://anyscale-public-materials.s3.us-west-2.amazonaws.com/ray-ai-libraries/diagrams/autoscaling_at_0.png' width=700/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "761fd6a6",
   "metadata": {},
   "source": [
    "We can send out a larger number of requests to the `OnlineMNISTClassifier` deployment to see the autoscaling in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7a834f",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = {\"image\": np.random.rand(10, 1, 28, 28)}\n",
    "[\n",
    "    mnist_deployment_handle.predict.remote(batch)\n",
    "    for _ in range(100)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a4e5e9",
   "metadata": {},
   "source": [
    "Looking at the Ray Serve dashboard, we can see that the number of replicas has scaled up to 10 as expected.\n",
    "\n",
    "<img src='https://anyscale-public-materials.s3.us-west-2.amazonaws.com/ray-ai-libraries/diagrams/autoscaling_at_10.png' width=700/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52225df",
   "metadata": {},
   "source": [
    "Let's shutdown the service for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0d53e06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "serve.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2d1c58",
   "metadata": {},
   "source": [
    "### Composing Deployments\n",
    "\n",
    "Ray Serve allows us to compose Deployments together to build more complex applications.\n",
    "\n",
    "Lets compose our `OnlineMNISTClassifier` with an `OnlineMNISTPreprocessor` deployment that performs the necessary transformations on the input data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67670984",
   "metadata": {},
   "outputs": [],
   "source": [
    "@serve.deployment\n",
    "class OnlineMNISTPreprocessor:\n",
    "    def __init__(self):\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5,), (0.5,))\n",
    "        ])\n",
    "        \n",
    "    async def run(self, batch: dict[str, Any]) -> dict[str, Any]:\n",
    "        images = batch[\"image\"]\n",
    "        images = [self.transform(np.array(image, dtype=np.uint8)).cpu().numpy() for image in images]\n",
    "        return {\"image\": images}\n",
    "\n",
    "preprocessor_app = OnlineMNISTPreprocessor.bind()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0dc24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor_handle = serve.run(preprocessor_app, name='mnist_preprocessor', blocking=False, route_prefix=\"/preprocess\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92daf899",
   "metadata": {},
   "source": [
    "Let's load an image and pass it to the `ImageTransformDeployment`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441a8762",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ray.data.read_images(\"s3://anyscale-public-materials/ray-ai-libraries/mnist/50_per_index/\", include_paths=True)\n",
    "image_batch = ds.take_batch(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1289797c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the first image using matplotlib\n",
    "plt.imshow(image_batch[\"image\"][0], cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94df4e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_batch = await preprocessor_handle.run.remote(image_batch)\n",
    "\n",
    "for image in normalized_batch[\"image\"]:\n",
    "    assert image.shape == (1, 28, 28) # channel, height, width\n",
    "    assert image.min() >= -1 and image.max() <= 1 # normalized to [-1, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2848fc",
   "metadata": {},
   "source": [
    "We will proceed to shutdown the preprocessor application to prove it will be automatically created by the ingress.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ac5957",
   "metadata": {},
   "outputs": [],
   "source": [
    "serve.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e44763",
   "metadata": {},
   "source": [
    "Let's now build an ingress for our application that composes the `ImageTransformDeployment` and `OnlineMNISTClassifier`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88340028",
   "metadata": {},
   "outputs": [],
   "source": [
    "@serve.deployment\n",
    "class ImageServiceIngress:\n",
    "    def __init__(self, preprocessor: OnlineMNISTPreprocessor, model: OnlineMNISTClassifier):\n",
    "        self.preprocessor = preprocessor\n",
    "        self.model = model\n",
    "\n",
    "    async def __call__(self, request: Request):\n",
    "        batch = json.loads(await request.json())\n",
    "        response = await self.preprocessor.run.remote(batch)\n",
    "        return await self.model.predict.remote(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affcac11",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_classifier_ingress = ImageServiceIngress.bind(\n",
    "    preprocessor=OnlineMNISTPreprocessor.bind(),\n",
    "    model=OnlineMNISTClassifier.options(\n",
    "        num_replicas=1,\n",
    "        ray_actor_options={\"num_gpus\": 0.1},\n",
    "    ).bind(local_path=\"/mnt/cluster_storage/model.pt\"),\n",
    ")\n",
    "\n",
    "handle = serve.run(image_classifier_ingress, name='image_classifier', blocking=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa81a51f",
   "metadata": {},
   "source": [
    "Let's test the application by sending a sample HTTP request to our ingress endpoint.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d084ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_request = json.dumps({\"image\": image_batch[\"image\"].tolist()}) \n",
    "response = requests.post(\"http://localhost:8000/\", json=json_request)\n",
    "response.json()[\"predicted_label\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fbe8773",
   "metadata": {},
   "source": [
    "### Integrating with FastAPI\n",
    "\n",
    "Ray Serve can be integrated with FastAPI to provide:\n",
    "- HTTP routing\n",
    "- Pydantic model validation\n",
    "- OpenAPI documentation\n",
    "\n",
    "To integrate a Deployment with FastAPI, we can use the `@serve.ingress` decorator to designate a FastAPI app as the entrypoint for HTTP requests to our Serve application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d163431",
   "metadata": {},
   "outputs": [],
   "source": [
    "app = FastAPI()\n",
    "\n",
    "@serve.deployment\n",
    "@serve.ingress(app)\n",
    "class ImageServiceIngress:\n",
    "    def __init__(self, preprocessor: OnlineMNISTPreprocessor, model: OnlineMNISTClassifier):\n",
    "        self.preprocessor = preprocessor\n",
    "        self.model = model\n",
    "    \n",
    "    @app.post(\"/predict\")\n",
    "    async def predict(self, request: Request):\n",
    "        batch = json.loads(await request.json())\n",
    "        response = await self.preprocessor.run.remote(batch)\n",
    "        out = await self.model.predict.remote(response)\n",
    "        return {\"predicted_label\": out[\"predicted_label\"].tolist()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a31b87",
   "metadata": {},
   "source": [
    "We now can build the application and run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0371807e",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_classifier_ingress = ImageServiceIngress.bind(\n",
    "    preprocessor=OnlineMNISTPreprocessor.bind(),\n",
    "    model=OnlineMNISTClassifier.options(\n",
    "        num_replicas=1,\n",
    "        ray_actor_options={\"num_gpus\": 0.1},\n",
    "    ).bind(local_path=\"/mnt/cluster_storage/model.pt\"),\n",
    ")\n",
    "\n",
    "handle = serve.run(image_classifier_ingress, name='image_classifier', blocking=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012894c6",
   "metadata": {},
   "source": [
    "After running the application, we can get test it as an HTTP endpoint programmatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e217336",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_request = json.dumps({\"image\": image_batch[\"image\"].tolist()}) \n",
    "response = requests.post(\"http://localhost:8000/predict\", json=json_request)\n",
    "response.json()[\"predicted_label\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "287ff14a",
   "metadata": {},
   "source": [
    "We can also visit the auto-generated FastAPI docs at http://localhost:8000/docs to get an interactive UI to test our endpoint."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2af689",
   "metadata": {},
   "source": [
    "## 4. Ray Serve in Production\n",
    "\n",
    "1. Klaviyo built their model serving platform with Ray Serve. See [this article from Klaviyo Engineering](https://klaviyo.tech/how-klaviyo-built-a-robust-model-serving-platform-with-ray-serve-c02ec65788b3)\n",
    "2. Samsara uses Ray Serve to bridge the gap of development to deployment of their models. See [this article from Samsara Engineering](https://www.samsara.com/blog/building-a-modern-machine-learning-platform-with-ray)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59f4a09",
   "metadata": {},
   "source": [
    "## Clean up \n",
    "\n",
    "Let's shutdown the application and clean up the resources we created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e8131d",
   "metadata": {},
   "outputs": [],
   "source": [
    "serve.shutdown()\n",
    "!rm -rf /mnt/cluster_storage/model.pt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
