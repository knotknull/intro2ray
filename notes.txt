
Ray AI Libraries


http://localhost:8888/notebooks/1_AI_Libs_Intro.ipynb


## Lesson 1:  Ray AI Libraries

5 parts to Ray 


[ Ray Data ] [ Ray Train ] [ Ray Tune ] [ Ray Serve ]
[ -------------------- Ray Core ------------------- ]


Key parts in the above notebook: 


## Read the dataset
dataset = ray.data.read_parquet("s3://anonymous@anyscale-training-data/intro-to-ray-air/nyc_taxi_2021.parquet")

## Split the dataset into training and validation sets
train_dataset, valid_dataset = dataset.train_test_split(test_size=0.3)


## Define the trainer
trainer = XGBoostTrainer(
    label_column="is_big_tip",
    scaling_config=ScalingConfig(num_workers=4, use_gpu=True),
    params={"objective": "binary:logistic"},
    datasets={"train": train_dataset, "valid": valid_dataset},
    run_config=RunConfig(storage_path="/mnt/cluster_storage/"),
)

## Fit the trainer
result = trainer.fit()


## Define the tuner
tuner = Tuner(
    trainer,
    param_space={"params": {"max_depth": tune.randint(2, 12)}},
    tune_config=TuneConfig(num_samples=3, metric="valid-logloss", mode="min"),
    run_config=RunConfig(storage_path="/mnt/cluster_storage/"),
)

## Fit the tuner and get the best checkpoint
checkpoint = tuner.fit().get_best_result().checkpoint

## Create an offline predictor 
class OfflinePredictor:
        # Load expensive state

## Apply the predictor to the validation dataset 
valid_dataset_inputs = valid_dataset.drop_columns(['is_big_tip'])
predicted_probabilities = valid_dataset_inputs.map_batches(OfflinePredictor, concurrency=2)

## Materialize a batch
predicted_probabilities.take_batch()

## Online prediction w/ Ray Serve
@serve.deployment
class OnlinePredictor:
    def __init__(self, checkpoint):
        ....
    async def __call__(self, request: Request) -> dict:
        # Handle HTTP request
        ....
    def predict(self, data: list[dict]) -> list[float]:
        # Make prediction
        ....

## Run the deployment
handle = serve.run(OnlinePredictor.bind(checkpoint=checkpoint))

## Get payload
valid_dataset_inputs = valid_dataset.drop_columns(["is_big_tip"])
sample_batch = valid_dataset_inputs.take_batch(1)
data = pd.DataFrame(sample_batch).to_json(orient="records")

## Send HTTP request
requests.post("http://localhost:8000/", json=data).json()


In overview the above:
 - gets parquet data and splits to train and valid subsets
 - sets a trainer and fits it 
 - sets a tuner  and fits it
 - creates an offline predictor
    - applies predictor to valid subset
    - gets prediction back
 - creates an online predictor with ray server
    - get a dataset from valid subset 
    - creates a dataframe and sends to online predictor







## Lesson 2:  Ray Train w/ PyTorch
http://localhost:8888/notebooks/2_Intro_Train.ipynb

Single GPU PyTorch fitting ResNet18 model to MNIST dataset
 - Training Process 

+------------------------------------------------------------------------+
|                                                                        |
|        +-----------------------------------------------------------+   |
|        |                                                           |   |
|        |                 +-----------------------------+           |   |
|        |                 v                             |           |   |
| [Data] |  [Model] --> [Forward] --> [Backward] --> [Update ]       |   |
|        |              [  Pass ]     [  Pass  ]     [Weights]   GPU |   |
|        |                                                           |   |
|        +-----------------------------------------------------------+   |
|                                                                        |
|                                                                        |
|                                                                   CPU  |
+------------------------------------------------------------------------+


The lesson takes you thru setting up data and a model to train on a local node and then 
how this functionality looks utilizing Ray and multiple workers and gpus

i.e. 
scaling_config = ScalingConfig(num_workers=2, use_gpu=True)


Distributed Data Parallel Training with Ray Train and PyTorch

+-----------------------------------------------------------------------------------+
|                               train.torch.prepare_model                           |
|         +---------------------------------------------------------------------+   |
|         |                                                                     |   |
|         |                 +-------------------------------------+             |   |
|         |                 v                                     |             |   |          train.report
| [Data]->|  [Model] --> [Forward] --> [Backward] --> [All   ] --> [Update ]    |   |
| [Shard] |    ^         [  Pass ]     [  Pass  ]     [Reduce] --> [Weights]    |   +------> [Model Checkpoint ]
|    |    |    |                                         ^                      |   |        [ and metrics     ]
|    |    |    |                                         |                GPU 0 |   |
|    |    +----+-----------------------------------------+----------------------+   |
|    |         |                                         |                          |
|    |       Sync                                      Sync                         |
| [Dataset]  initial state                             Gradients                    |
|    |         |                                          |                         |
|    |         |                                          |                         |
|    |    +----+-----------------------------------------------+----------------+   |
|    |    |    |                                          |                     |   |
|    |    |    v                                          v                     |   |
| [Data]->|  [Model] --> [Forward] --> [Backward] --> [ All  ] -->  [Update ]   |   |
| [Shard] |              [  Pass ]     [  Pass  ]     [Reduce] -->  [Weights]   |   |
|         |                 ^                                          |        |   |
|         |                 +------------------------------------------+        |   |
|         |                                                               GPU 1 |   |
|         +---------------------------------------------------------------------+   |
|                                                                                   |
|train.torch.prepare_data_loader                                                    |
|                                                                              CPU  |
+-----------------------------------------------------------------------------------+


The lesson takes you thru setting up data and a model to train on a local node and then 

## Lesson 3:  Ray Tune
notebooks/3_Intro_Tune.ipynb

Hyperparameter tuning library

MNIST Dataset : 28 x 28 grayscale (0=Black, 255=White) images of handwritten digits (0-9)

Training set: 60k images 
Test set    : 10k images 



## DataLoader from Torch

def build_data_loader(batch_size: int) -> torch.utils.data.DataLoader:
    transform = Compose([ToTensor(), Normalize((0.5,), (0.5,))])
    train_data = MNIST(root="./data", train=True, download=True, transform=transform)
    data_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, drop_last=True)
    return data_loader


Train loop utilizes:
  - resnet18 (18 layer deep network)
  - training loop (train_loop_torch)
  - # of epochs,  batch size, learning rate (100000)
  - put model onto gpu (  model.to("cuda")  )




## train_loop with number of epochs, batch size and learn rate 
## 
def train_loop_torch(num_epochs: int = 2, batch_size: int = 128, lr: float = 1e-5):   
    criterion = CrossEntropyLoss()                  ## how to calculate loss actual vs. estimated

    model = resnet18()                              ## Resnet18 Model
    model.conv1 = torch.nn.Conv2d(
        1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False        ## first layer of Resnet18 model needs to be "swapped out"
    )
    model.to("cuda")                                        ## put model on GPU 
    data_loader = build_data_loader(batch_size)             ## load data
    optimizer = Adam(model.parameters(), lr=lr)             ## optimizer with model params

    for epoch in range(num_epochs):
        for images, labels in data_loader:                          ## loop thru images, labels
            images, labels = images.to("cuda"), labels.to("cuda")   ## put onto GPU
            outputs = model(images)                                 ## predict output
            loss = criterion(outputs, labels)                       ## calculate loss
            optimizer.zero_grad()                                   ## set optimizer state
            loss.backward()                                         ## backward propogate loss
            optimizer.step()                                        ## take optimization step

        # Report the metrics
        print(f"Epoch {epoch}, Loss: {loss}")





Can process be done better? 

Hyperparameter tuning is a computationally expensive task, and it will take a long time to run sequentially.

Ray Tune is a distributed hyperparameter tuning library that can help us speed up the process!





recap what actually happened here ?

tuner = tune.Tuner(
    trainable=train_my_simple_model,  # Training function or class to be tuned
    param_space={
        "a": tune.randint(0, 20),  # Hyperparameter: a
    },
    tune_config=tune.TuneConfig(
        metric="rmse",  # Metric to optimize (minimize)
        mode="min",     # Minimize the metric
        num_samples=5,  # Number of samples to try
    ),
)

results = tuner.fit()

A Tuner accepts:

    A training function or class which is specified by trainable
    A search space which is specified by param_space
    A metric to optimize which is specified by metric and the direction of optimization mode
    num_samples which correlates to the number of trials to run

tuner.fit then runs multiple trials in parallel, each with a different set of hyperparameters, and returns the 
best set of hyperparameters found.



by default:

    Each trial will run in a separate process and consume 1 CPU core.
    Ray Tune uses a search algorithm to decide which trials to run next.
    Ray Tune uses a scheduler to decide if/when to stop trials, or to prioritize certain trials over others.


tuner = tune.Tuner(
    # This is how to specify resources for your trainable function
    trainable=tune.with_resources(train_my_simple_model, {"cpu": 1}),
    param_space={"a": tune.randint(0, 20)},
    tune_config=tune.TuneConfig(
        mode="min",
        metric="rmse",
        num_samples=5, 
        # This search algorithm is a basic variation (i.e random/grid search) based on parameter space
        search_alg=tune.search.BasicVariantGenerator(), 
        # This scheduler is very simple: no early stopping, just run all trials in submission order
        scheduler=tune.schedulers.FIFOScheduler(), 
    ),
)
results = tuner.fit()

Ray Tune components 

   [ Search Spaces    ]   [ Trainables  Spaces      ]
   [ Which parameters ]   [ which objective to tune ] --+
                                                        |
                          [ Search Algorithms ]         +--------> [ Trials          ]--------> [ Search Algorithms ]
                          [ How to Tune       ]         |          [ Run experiments ]          [ How to Tune       ]
                                                        |
                          [ Schedulers   ]              |
                          [ When to Stop ]            --+






Hyperparameter tune the PyTorch model using Ray Tune

The first step is to move in all the PyTorch code into a function that we can pass to the trainable argument of the tune.run function.


def train_pytorch(config): # we change the function so it accepts a config dictionary
    criterion = CrossEntropyLoss()

    model = resnet18()
    model.conv1 = torch.nn.Conv2d(
        1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
    )
    model.to("cuda")

    optimizer = Adam(model.parameters(), lr=config["lr"])
    transform = Compose([ToTensor(), Normalize((0.5,), (0.5,))])                            ## Ray specific
    train_data = MNIST(root="./data", train=True, download=True, transform=transform)       ## Ray specific
    data_loader = DataLoader(train_data, batch_size=config["batch_size"], shuffle=True, drop_last=True)  ## Ray specific

    for epoch in range(config["num_epochs"]):
        for images, labels in data_loader:
            images, labels = images.to("cuda"), labels.to("cuda")
            outputs = model(images)
            loss = criterion(outputs, labels)
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

        # Report the metrics using train.report instead of print
        train.report({"loss": loss.item()}                 ## Ray specific)




The second and third steps are the same as before. We define the tuner and run it by calling the fit method.

tuner = tune.Tuner(
    trainable=tune.with_resources(train_pytorch, {"gpu": 1}),   # Ray Tune: we will dedicate 1 GPU to each trial
    param_space={                                               # Ray Tune specific 
        "num_epochs": 1,
        "batch_size": 128,
        "lr": tune.loguniform(1e-4, 1e-1),
    },
    tune_config=tune.TuneConfig(
        mode="min",
        metric="loss",
        num_samples=2,
        search_alg=tune.search.BasicVariantGenerator(),
        scheduler=tune.schedulers.FIFOScheduler(),
    ),
)

results = tuner.fit()

best_result = results.get_best_result()
best_result.config





https://www.uber.com/blog/from-predictive-to-generative-ai/

https://engineering.atspotify.com/2023/02/unleashing-ml-innovation-at-spotify-with-ray/





## Lesson 4:  Ray Data
notebooks/4_Intro_Data.ipynb


Ray datasets are main abstraction for Ray Data


Use Datasets as a "last-mile bridge" from storage or ETL pipeline outputs to distributed applications and libraries in Ray. 
                               +----------------------------------------------+ 
[ DataFrame / Relational ]  ->  | [ Load Data / Last Mile ] -> [  ML      ]   |
[ Data Processing / ETL  ]      | [ Preprocessing         ]    [ Training ]   |
                                +---------------------------------------------+
                                            ^      Ray Cluster     ^
                                 Ray Data --+                      +---- Ray Train



Loading Data:
Datasets use Ray tasks to read data from remote storage.  When reading from file-based datasource (S3, GCS)
it creates a number of read tasks proportional to the number of CPUS in the cluster.

Each read task is assigned files and produces an output block: 

[ File 1 ] -->> [Read Task] -->> [ Block ]
[ File 2 ] -->> [Read Task] -->> [ Block ]
[ File 3 ] -->> [Read Task] -->> [ Block ]
[ File 4 ] -->> [Read Task] -->> [ Block ]

ray.data.read_csv(path)

# Sample

!aws s3 ls s3://anyscale-public-materials/ray-ai-libraries/mnist/50_per_index/
ds = ray.data.read_images("s3://anyscale-public-materials/ray-ai-libraries/mnist/50_per_index/", include_paths=True)
ds


NOTE: Ray  Data API  
https://docs.ray.io/en/latest/data/api/input_output.html


Datasets consist of blocks
 - a list of Ray object references to blocks 
    - numpy arrays

ray.data.Dataset
            |  "col"  | "col2"
------------|---------|-------------
   1 - 1000 | ObjRef(block1)  ------------->  pyarrow.Table [ num_rows=1000, schema={"col1", "col2"} ]  
1001 - 2000 | ObjRef(block2)  ------------->  pyarrow.Table [ num_rows=1000, schema={"col1", "col2"} ]  
2001 - 3000 | ObjRef(block3)  ------------->  pyarrow.Table [ num_rows=1000, schema={"col1", "col2"} ]  



each block is a pyarrow.Table with X rows and Schema

A dataset or individual block can be processed by Ray task or actor
NOTE: Using actors allows for expensive state initialization (e.g., for GPU-based tasks) to be cached.


 +----------------------------------------------------------------------------------------+
 |                                                                                        |
 |                         [ Actor Pool  ]      [ Actor Pool  ]                           |
 |  [ CPU Tasks     ]      [ GPU Tasks   ]      [ GPU Tasks   ]      [ CPU Tasks    ]     |                                                            |                                                                |                                                            |                                                             |                                                            |
 |  [S3 Get-> Resize] -+-> [ Segmentation] -+-> [ Segmentation] -+-> [ Write Result ]     |
 |  [S3 Get-> Resize]  |   [ Model       ]  |   [ Model       ]  |   [ Write Result ]     |                                                                                   |
 |                     |                    |                    |                        |
 |                     |                    |                    |                        |
 +---------------------|--------------------|--------------------|------------------------+
                       |                    |                    |          Ray Cluster
                       +--------------------+--------------------+
                                            |
                                Data streamed through memory        


Ray Tasks for stateless operations and Ray Actors for stateful operations

i.e. Load a large ML Model once and pass batches over it.
    - can execute data transformation on mix of GPUs and CPUS


map_batches allows for transformation on batches of data: 

def normalize(
    batch: dict[str, np.ndarray], min_: float, max_: float
) -> dict[str, np.ndarray]:
    transform = Compose([ToTensor(), Normalize((0.5,), (0.5,))])
    batch["image"] = [transform(image) for image in batch["image"]]
    return batch


ds_normalized = ds.map_batches(normalize, fn_kwargs={"min_": 0, "max_": 255})
ds_normalized



Execution mode

Most transformations are lazy. They don't execute until you write a dataset to storage or decide to materialize/consume the dataset.

NOTE: To materialize a very small subset of the data, you can use the take_batch method.




Stateful transformations with actors

In cases like batch inference, you want to spin up a number of actor processes that are initialized once 
with your model and reused to process multiple batches.

To implement this, you can use the map_batches API with a "Callable" class method that implements:

    __init__: Initialize any expensive state.
    __call__: Perform the stateful transformation.

For example, we can implement a MNISTClassifier that:

    loads a pre-trained model from a local file
    accepts a batch of images and generates the predicted label


class MNISTClassifier:
    def __init__(self, local_path: str):
        self.model = torch.jit.load(local_path)
        self.model.to("cuda")
        self.model.eval()

    def __call__(self, batch: dict[str, np.ndarray]) -> dict[str, np.ndarray]:
        images = torch.tensor(batch["image"]).float().to("cuda")

        with torch.no_grad():
            logits = self.model(images).cpu().numpy()

        batch["predicted_label"] = np.argmax(logits, axis=1)
        return batch

# We download the model from s3 to an EFS storage
!aws s3 cp s3://anyscale-public-materials/ray-ai-libraries/mnist/model/model.pt /mnt/cluster_storage/model.pt

ds_preds = ds_normalized.map_batches(
    MNISTClassifier,
    fn_constructor_kwargs={"local_path": "/mnt/cluster_storage/model.pt"},
    num_gpus=0.1,
    concurrency=1,
    batch_size=100,
)

NOTE:  Call map_batches with MNISTClassifier Callable API class




Can materialize data set to ray object store distributed across cluster via materialize() 
NOTE; only use when you require full dataset to compute downstream outputs



Data Operations: Grouping, Aggregation and Shuffling


Use groupby() to generate batches by specific key followed by map_groups for transformation




# set ground truth label of picture
# 
def add_label(batch: dict[str, np.ndarray]) -> dict[str, np.ndarray]:
    batch["ground_truth_label"] = [int(path.split("/")[-2]) for path in batch["path"]]
    return batch

# set ground truth label of picture
#  predicted label vs ground truthe label  
#  
def compute_accuracy(group: dict[str, np.ndarray]) -> dict[str, np.ndarray]:
    return {
        "accuracy": [np.mean(group["predicted_label"] == group["ground_truth_label"])],
        "ground_truth_label": group["ground_truth_label"][:1],
    }


# create batches, grouped by groud truth label and then groups mapped to accuracy
ds_preds.map_batches(add_label).groupby("ground_truth_label").map_groups(compute_accuracy).to_pandas()


# Aggregates

Ray Data provides the following agg functions: 
- count
- max
- mean
- min
- sum
- std

ds_preds.map_batches(add_label).map_batches(compute_accuracy).mean(on="accuracy")


# Shuffling Data

Shuffle based on file reads


To randomly shuffle the ordering of input files before reading, call a read function that supports shuffling, 
such as `read_images()`, and use the shuffle="files" parameter.

ray.data.read_images("s3://anyscale-public-materials/ray-ai-libraries/mnist/50_per_index/", shuffle="files")


Shuffling block order

Randomizes the order of blocks in a dataset. Applying this operation alone doesn’t involve heavy computation and communication. 
However, it requires Ray Data to materialize all blocks in memory before applying the operation. Only use this option when your 
dataset is small enough to fit into the object store memory.

To perform block order shuffling, use randomize_block_order.

ds_randomized_blocks = ds_preds.randomize_block_order()
ds_randomized_blocks.materialize()


Shuffle all rows globally

To randomly shuffle all rows globally, call random_shuffle(). This is the slowest option for shuffle, 
and requires transferring data across network between workers. 
This option achieves the best randomness among all options but is expensive due to communication across cluster.

ds_randomized_rows = ds_preds.random_shuffle()
ds_randomized_rows.materialize()


# Persist data using Ray Data write functions 

ds_preds.write_parquet("/mnt/cluster_storage/mnsit_preds")


NOTE: Ray Data API for I/O
https://docs.ray.io/en/latest/data/api/input_output.html

Ray Data in Production: 


https://siliconangle.com/2024/10/02/runway-transforming-ai-driven-filmmaking-innovative-tools-techniques-raysummit/

https://raysummit.anyscale.com/flow/anyscale/raysummit2024/landing/page/sessioncatalog/session/1722028596844001bCg0




## Lesson 5:  Ray Serve
notebooks/5_Intro_Serve.ipynb























.