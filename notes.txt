
Ray AI Libraries


http://localhost:8888/notebooks/1_AI_Libs_Intro.ipynb


## Lesson 1:  Ray AI Libraries

5 parts to Ray 


[ Ray Data ] [ Ray Train ] [ Ray Tune ] [ Ray Serve ]
[ -------------------- Ray Core ------------------- ]


Key parts in the above notebook: 


## Read the dataset
dataset = ray.data.read_parquet("s3://anonymous@anyscale-training-data/intro-to-ray-air/nyc_taxi_2021.parquet")

## Split the dataset into training and validation sets
train_dataset, valid_dataset = dataset.train_test_split(test_size=0.3)


## Define the trainer
trainer = XGBoostTrainer(
    label_column="is_big_tip",
    scaling_config=ScalingConfig(num_workers=4, use_gpu=True),
    params={"objective": "binary:logistic"},
    datasets={"train": train_dataset, "valid": valid_dataset},
    run_config=RunConfig(storage_path="/mnt/cluster_storage/"),
)

## Fit the trainer
result = trainer.fit()


## Define the tuner
tuner = Tuner(
    trainer,
    param_space={"params": {"max_depth": tune.randint(2, 12)}},
    tune_config=TuneConfig(num_samples=3, metric="valid-logloss", mode="min"),
    run_config=RunConfig(storage_path="/mnt/cluster_storage/"),
)

## Fit the tuner and get the best checkpoint
checkpoint = tuner.fit().get_best_result().checkpoint

## Create an offline predictor 
class OfflinePredictor:
        # Load expensive state

## Apply the predictor to the validation dataset 
valid_dataset_inputs = valid_dataset.drop_columns(['is_big_tip'])
predicted_probabilities = valid_dataset_inputs.map_batches(OfflinePredictor, concurrency=2)

## Materialize a batch
predicted_probabilities.take_batch()

## Online prediction w/ Ray Serve
@serve.deployment
class OnlinePredictor:
    def __init__(self, checkpoint):
        ....
    async def __call__(self, request: Request) -> dict:
        # Handle HTTP request
        ....
    def predict(self, data: list[dict]) -> list[float]:
        # Make prediction
        ....

## Run the deployment
handle = serve.run(OnlinePredictor.bind(checkpoint=checkpoint))

## Get payload
valid_dataset_inputs = valid_dataset.drop_columns(["is_big_tip"])
sample_batch = valid_dataset_inputs.take_batch(1)
data = pd.DataFrame(sample_batch).to_json(orient="records")

## Send HTTP request
requests.post("http://localhost:8000/", json=data).json()


In overview the above:
 - gets parquet data and splits to train and valid subsets
 - sets a trainer and fits it 
 - sets a tuner  and fits it
 - creates an offline predictor
    - applies predictor to valid subset
    - gets prediction back
 - creates an online predictor with ray server
    - get a dataset from valid subset 
    - creates a dataframe and sends to online predictor







## Lesson 2:  Ray Train w/ PyTorch
http://localhost:8888/notebooks/2_Intro_Train.ipynb

Single GPU PyTorch fitting ResNet18 model to MNIST dataset
 - Training Process 

+------------------------------------------------------------------------+
|                                                                        |
|        +-----------------------------------------------------------+   |
|        |                                                           |   |
|        |                 +-----------------------------+           |   |
|        |                 v                             |           |   |
| [Data] |  [Model] --> [Forward] --> [Backward] --> [Update ]       |   |
|        |              [  Pass ]     [  Pass  ]     [Weights]   GPU |   |
|        |                                                           |   |
|        +-----------------------------------------------------------+   |
|                                                                        |
|                                                                        |
|                                                                   CPU  |
+------------------------------------------------------------------------+


The lesson takes you thru setting up data and a model to train on a local node and then 
how this functionality looks utilizing Ray and multiple workers and gpus

i.e. 
scaling_config = ScalingConfig(num_workers=2, use_gpu=True)


Distributed Data Parallel Training with Ray Train and PyTorch

+-----------------------------------------------------------------------------------+
|                                                                                   |
|         +---------------------------------------------------------------------+   |
|         |                                                                     |   |
|         |                 +-------------------------------------+             |   |
|         |                 v                                     |             |   |
| [Data]->|  [Model] --> [Forward] --> [Backward] --> [All   ] --> [Update ]    |   |
| [Shard] |    ^         [  Pass ]     [  Pass  ]     [Reduce] --> [Weights]    |   +------> [Model Checkpoint ]
|    |    |    |                                         ^                      |   |        [ and metrics     ]
|    |    |    |                                         |                  GPU |   |
|    |    +----+-----------------------------------------+----------------------+   |
|    |         |                                         |                          |
|    |       Sync                                      Sync                         |
| [Dataset]  initial state                             Gradients                    |
|    |         |                                          |                         |
|    |         |                                          |                         |
|    |    +----+-----------------------------------------------+----------------+   |
|    |    |    |                                          |                     |   |
|    |    |    v                                          v                     |   |
| [Data]->|  [Model] --> [Forward] --> [Backward] --> [ All  ] -->  [Update ]   |   |
| [Shard] |              [  Pass ]     [  Pass  ]     [Reduce] -->  [Weights]   |   |
|         |                 ^                                          |        |   |
|         |                 +------------------------------------------+        |   |
|         |                                                                 GPU |   |
|         +---------------------------------------------------------------------+   |
|                                                                                   |
|                                                                                   |
|                                                                              CPU  |
+-----------------------------------------------------------------------------------+


The lesson takes you thru setting up data and a model to train on a local node and then 

.